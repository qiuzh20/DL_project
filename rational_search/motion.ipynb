{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/qiuzihan/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _nlv = LooseVersion(_np_version)\n",
      "/workspace/qiuzihan/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:11: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _np_version_under1p16 = _nlv < LooseVersion(\"1.16\")\n",
      "/workspace/qiuzihan/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:12: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _np_version_under1p17 = _nlv < LooseVersion(\"1.17\")\n",
      "/workspace/qiuzihan/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:13: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _np_version_under1p18 = _nlv < LooseVersion(\"1.18\")\n",
      "/workspace/qiuzihan/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:14: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _np_version_under1p19 = _nlv < LooseVersion(\"1.19\")\n",
      "/workspace/qiuzihan/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:15: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _np_version_under1p20 = _nlv < LooseVersion(\"1.20\")\n",
      "/workspace/qiuzihan/anaconda3/envs/nlp/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/workspace/qiuzihan/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/compat/numpy/function.py:125: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(_np_version) >= LooseVersion(\"1.17.0\"):\n",
      "/workspace/qiuzihan/anaconda3/envs/nlp/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "train_ds, dev_ds, test_ds = load_dataset(\"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-30 01:50:40,564] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-base-zh'.\u001b[0m\n",
      "\u001b[32m[2022-05-30 01:50:40,568] [    INFO]\u001b[0m - Already cached /home/qiuzihan/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams\u001b[0m\n",
      "W0530 01:50:40.572010 1318026 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2\n",
      "W0530 01:50:40.581347 1318026 gpu_context.cc:306] device: 0, cuDNN Version: 7.6.\n",
      "\u001b[32m[2022-05-30 01:50:47,544] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-base-zh'.\u001b[0m\n",
      "\u001b[32m[2022-05-30 01:50:47,547] [    INFO]\u001b[0m - Already cached /home/qiuzihan/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"ernie-3.0-base-zh\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=len(train_ds.label_list))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "\n",
    "# 数据预处理函数，利用分词器将文本转化为整数序列\n",
    "def preprocess_function(examples, tokenizer, max_seq_length, is_test=False):\n",
    "\n",
    "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
    "    if not is_test:\n",
    "        result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128)\n",
    "train_ds = train_ds.map(trans_func)\n",
    "dev_ds = dev_ds.map(trans_func)\n",
    "\n",
    "# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "dev_batch_sampler = BatchSampler(dev_ds, batch_size=32, shuffle=False)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "dev_data_loader = DataLoader(dataset=dev_ds, batch_sampler=dev_batch_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, metric, dataloader):\n",
    "    label_map = {0: '负面', 1: '正面'}\n",
    "    results = []\n",
    "    model.eval()\n",
    "    eval_metric = paddle.metric.Accuracy()\n",
    "    for batch in dataloader:\n",
    "        logits = model(batch['input_ids'], batch['token_type_ids'])\n",
    "        probs = F.softmax(logits, axis=-1)\n",
    "        correct = eval_metric.compute(probs, batch['labels'])\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 20, epoch: 1, batch: 20, loss: 0.62837, accu: 0.63594, speed: 1.97 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.34484, accu: 0.74766, speed: 2.14 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.35660, accu: 0.78594, speed: 2.15 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.30064, accu: 0.81328, speed: 2.15 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.17698, accu: 0.83313, speed: 2.15 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.42386, accu: 0.84401, speed: 2.15 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.28324, accu: 0.85446, speed: 2.15 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.41796, accu: 0.86172, speed: 2.13 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.34469, accu: 0.86528, speed: 2.06 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.38285, accu: 0.86766, speed: 2.00 step/s\n",
      "200 eval acc: 0.8681592039800995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-30 01:52:04,807] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-05-30 01:52:04,809] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 220, epoch: 1, batch: 220, loss: 0.07424, accu: 0.87359, speed: 1.20 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.14120, accu: 0.87707, speed: 1.99 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.16949, accu: 0.88087, speed: 1.99 step/s\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.17400, accu: 0.88301, speed: 1.98 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.25334, accu: 0.88590, speed: 1.99 step/s\n",
      "global step 320, epoch: 2, batch: 20, loss: 0.08075, accu: 0.88931, speed: 1.95 step/s\n",
      "global step 340, epoch: 2, batch: 40, loss: 0.02499, accu: 0.89223, speed: 1.99 step/s\n",
      "global step 360, epoch: 2, batch: 60, loss: 0.08604, accu: 0.89552, speed: 1.98 step/s\n",
      "global step 380, epoch: 2, batch: 80, loss: 0.05529, accu: 0.89870, speed: 1.98 step/s\n",
      "global step 400, epoch: 2, batch: 100, loss: 0.20544, accu: 0.90181, speed: 1.98 step/s\n",
      "400 eval acc: 0.9018967661691543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-30 01:52:58,534] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-05-30 01:52:58,536] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 420, epoch: 2, batch: 120, loss: 0.08471, accu: 0.90418, speed: 1.19 step/s\n",
      "global step 440, epoch: 2, batch: 140, loss: 0.17566, accu: 0.90632, speed: 1.98 step/s\n",
      "global step 460, epoch: 2, batch: 160, loss: 0.06226, accu: 0.90923, speed: 1.98 step/s\n",
      "global step 480, epoch: 2, batch: 180, loss: 0.10699, accu: 0.91124, speed: 1.98 step/s\n",
      "global step 500, epoch: 2, batch: 200, loss: 0.04766, accu: 0.91272, speed: 1.99 step/s\n",
      "global step 520, epoch: 2, batch: 220, loss: 0.47114, accu: 0.91445, speed: 1.99 step/s\n",
      "global step 540, epoch: 2, batch: 240, loss: 0.13604, accu: 0.91599, speed: 1.99 step/s\n",
      "global step 560, epoch: 2, batch: 260, loss: 0.05447, accu: 0.91765, speed: 1.98 step/s\n",
      "global step 580, epoch: 2, batch: 280, loss: 0.17768, accu: 0.91849, speed: 1.99 step/s\n",
      "global step 600, epoch: 2, batch: 300, loss: 0.08136, accu: 0.92001, speed: 1.99 step/s\n",
      "600 eval acc: 0.9201388888888888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-30 01:53:52,272] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-05-30 01:53:52,274] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 620, epoch: 3, batch: 20, loss: 0.01826, accu: 0.92205, speed: 1.17 step/s\n",
      "global step 640, epoch: 3, batch: 40, loss: 0.16683, accu: 0.92413, speed: 1.99 step/s\n",
      "global step 660, epoch: 3, batch: 60, loss: 0.00969, accu: 0.92609, speed: 1.99 step/s\n",
      "global step 680, epoch: 3, batch: 80, loss: 0.01197, accu: 0.92785, speed: 1.99 step/s\n",
      "global step 700, epoch: 3, batch: 100, loss: 0.01086, accu: 0.92941, speed: 1.99 step/s\n",
      "global step 720, epoch: 3, batch: 120, loss: 0.03129, accu: 0.93076, speed: 1.99 step/s\n",
      "global step 740, epoch: 3, batch: 140, loss: 0.01876, accu: 0.93220, speed: 1.99 step/s\n",
      "global step 760, epoch: 3, batch: 160, loss: 0.00664, accu: 0.93357, speed: 1.99 step/s\n",
      "global step 780, epoch: 3, batch: 180, loss: 0.01458, accu: 0.93499, speed: 1.99 step/s\n",
      "global step 800, epoch: 3, batch: 200, loss: 0.00516, accu: 0.93622, speed: 1.99 step/s\n",
      "800 eval acc: 0.9362950870646766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-30 01:54:46,131] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-05-30 01:54:46,136] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 820, epoch: 3, batch: 220, loss: 0.13002, accu: 0.93735, speed: 1.19 step/s\n",
      "global step 840, epoch: 3, batch: 240, loss: 0.14899, accu: 0.93809, speed: 1.99 step/s\n",
      "global step 860, epoch: 3, batch: 260, loss: 0.01138, accu: 0.93887, speed: 1.98 step/s\n",
      "global step 880, epoch: 3, batch: 280, loss: 0.03181, accu: 0.93966, speed: 1.99 step/s\n",
      "global step 900, epoch: 3, batch: 300, loss: 0.01262, accu: 0.94071, speed: 1.99 step/s\n",
      "global step 920, epoch: 4, batch: 20, loss: 0.19415, accu: 0.94169, speed: 1.95 step/s\n",
      "global step 940, epoch: 4, batch: 40, loss: 0.01577, accu: 0.94280, speed: 1.99 step/s\n",
      "global step 960, epoch: 4, batch: 60, loss: 0.07460, accu: 0.94395, speed: 1.99 step/s\n",
      "global step 980, epoch: 4, batch: 80, loss: 0.00344, accu: 0.94496, speed: 1.99 step/s\n",
      "global step 1000, epoch: 4, batch: 100, loss: 0.05695, accu: 0.94590, speed: 1.99 step/s\n",
      "1000 eval acc: 0.9459577114427861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-30 01:55:40,041] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-05-30 01:55:40,046] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1020, epoch: 4, batch: 120, loss: 0.00382, accu: 0.94689, speed: 1.18 step/s\n",
      "global step 1040, epoch: 4, batch: 140, loss: 0.00406, accu: 0.94773, speed: 1.98 step/s\n",
      "global step 1060, epoch: 4, batch: 160, loss: 0.00341, accu: 0.94868, speed: 1.98 step/s\n",
      "global step 1080, epoch: 4, batch: 180, loss: 0.01068, accu: 0.94940, speed: 1.99 step/s\n",
      "global step 1100, epoch: 4, batch: 200, loss: 0.04355, accu: 0.95011, speed: 1.99 step/s\n",
      "global step 1120, epoch: 4, batch: 220, loss: 0.00341, accu: 0.95083, speed: 1.99 step/s\n",
      "global step 1140, epoch: 4, batch: 240, loss: 0.03903, accu: 0.95134, speed: 1.99 step/s\n",
      "global step 1160, epoch: 4, batch: 260, loss: 0.00416, accu: 0.95198, speed: 1.98 step/s\n",
      "global step 1180, epoch: 4, batch: 280, loss: 0.02726, accu: 0.95258, speed: 1.98 step/s\n",
      "global step 1200, epoch: 4, batch: 300, loss: 0.00762, accu: 0.95319, speed: 1.98 step/s\n",
      "1200 eval acc: 0.9532027363184079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-30 01:56:33,982] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-05-30 01:56:33,984] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1220, epoch: 5, batch: 20, loss: 0.00448, accu: 0.95389, speed: 1.16 step/s\n",
      "global step 1240, epoch: 5, batch: 40, loss: 0.00217, accu: 0.95460, speed: 1.98 step/s\n",
      "global step 1260, epoch: 5, batch: 60, loss: 0.00207, accu: 0.95525, speed: 1.98 step/s\n",
      "global step 1280, epoch: 5, batch: 80, loss: 0.00651, accu: 0.95590, speed: 1.98 step/s\n",
      "global step 1300, epoch: 5, batch: 100, loss: 0.00189, accu: 0.95650, speed: 1.97 step/s\n",
      "global step 1320, epoch: 5, batch: 120, loss: 0.08823, accu: 0.95708, speed: 1.97 step/s\n",
      "global step 1340, epoch: 5, batch: 140, loss: 0.00297, accu: 0.95761, speed: 1.98 step/s\n",
      "global step 1360, epoch: 5, batch: 160, loss: 0.07320, accu: 0.95818, speed: 1.97 step/s\n",
      "global step 1380, epoch: 5, batch: 180, loss: 0.00122, accu: 0.95872, speed: 1.97 step/s\n",
      "global step 1400, epoch: 5, batch: 200, loss: 0.00103, accu: 0.95924, speed: 1.98 step/s\n",
      "1400 eval acc: 0.9592217484008528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-30 01:57:28,256] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-05-30 01:57:28,259] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1420, epoch: 5, batch: 220, loss: 0.01125, accu: 0.95973, speed: 1.17 step/s\n",
      "global step 1440, epoch: 5, batch: 240, loss: 0.00887, accu: 0.96018, speed: 1.98 step/s\n",
      "global step 1460, epoch: 5, batch: 260, loss: 0.00206, accu: 0.96061, speed: 1.99 step/s\n",
      "global step 1480, epoch: 5, batch: 280, loss: 0.00070, accu: 0.96106, speed: 1.99 step/s\n",
      "global step 1500, epoch: 5, batch: 300, loss: 0.00053, accu: 0.96151, speed: 1.99 step/s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "epochs = 5 # 训练轮次\n",
    "ckpt_dir = \"ernie_ckpt\" #训练过程中保存模型参数的文件夹\n",
    "best_acc = 0\n",
    "best_step = 0\n",
    "global_step = 0 #迭代次数\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # 计算模型输出、损失函数值、分类概率值、准确率\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        # 每迭代20次，打印损失函数值、准确率、计算速度\n",
    "        global_step += 1\n",
    "        if global_step % 20 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        # 每迭代200次，评估当前训练的模型、保存当前模型参数和分词器的词表等\n",
    "        if global_step % 200 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            print(global_step, end=' ')\n",
    "            acc_eval = evaluate(model, criterion, metric, dev_data_loader)\n",
    "            print(\"eval acc:\", acc_eval)\n",
    "            if acc_eval > best_acc:\n",
    "                best_acc = acc_eval\n",
    "                best_step = global_step\n",
    "\n",
    "                model.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNIE 3.0 在ChnSentiCorp的dev集表现 0.9614811133200796\n"
     ]
    }
   ],
   "source": [
    "params_path = 'ernie_ckpt/model_state.pdparams'\n",
    "state_dict = paddle.load(params_path)\n",
    "model.set_dict(state_dict)\n",
    "\n",
    "# 也可以选择加载预先训练好的模型参数结果查看模型训练结果\n",
    "# model.set_dict(paddle.load('ernie_ckpt_trained/model_state.pdparams'))\n",
    "\n",
    "print('ERNIE 3.0 在ChnSentiCorp的dev集表现', end=' ')\n",
    "eval_acc = evaluate(model, criterion, metric, dev_data_loader)\n",
    "print(eval_acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7518e244f6dc35dfad2eee09845a3d78cd9d2c7aebb0d0aa875d2ce868ca3770"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
