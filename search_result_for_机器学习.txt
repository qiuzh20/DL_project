标题：Moloco李子瞻谈“应用出海”:机器学习赋能数字营销
https://3w.huanqiu.com/a/c36dc8/48V6w9BlAkv
keywords  0 grammer  2 
get_summary
“伴随着数字化的浪潮，中国企业出海范围增大，全球化路径多点开花。中国企业持续创新引领潮流，出海的很多精品应用成为国外企业模仿的对象。”谈到中国应用出海的趋势，Moloco大中华区销售总监李子瞻（Bruce在过去的十余年间，中国移动应用的出海航程越走越宽、越走越远。但应用出海也并非一帆风顺，对于企业而言，更需要稳稳掌舵，乘风破浪。作为全球数字营销解决方案头部厂商，Moloco以机器学习算法为底座，帮助中国企业出海远航，并持续解锁新的增量空间。从2013年成立至今，Moloco一直注重技术架构的搭建。李子瞻告诉记者，“与底层技术偏向商务侧的广告平台不同，Moloco的广告基础架构就是用机器学习的模型搭建的，我们用到的是机器学习里面的深度学习、深度神经网络这套算法，公司从最开始的时候就选择了这条路线。”以机器学习为底座，也蕴藏着Moloco对行业的洞察。Moloco认为，目前数字经济快速发展，但也面临一系列挑战，涉及数据安全合规等层面，而机器学习可以帮助企业应对这些挑战，并赋能数字经济的发展。据李子瞻介绍，机器学习可以助力Moloco客户的发展更加合规、透明和高效。“Moloco的透明度都是在合规的前提下实现的，可做到一个App一个模型，且模型是用客户的一手数据来训练出来的，因此客户无需担心数据会泄露。”此外，基于机器学习算法，Moloco可以帮助更多的中国企业触及到海量的海外受众。“Moloco可以提供给中国客户很多精细化和定制化的数据，帮助他们找到合适的受众，进而通过这个程序化的广告生态帮助他们达成目标。在ROI完成方面，Moloco也可以实现较短的周期和较低的成本。”李子瞻说。得益于技术平台架构的灵活性，Moloco在面对相关政策的变化时也会更加有弹性。李子瞻举例称，“比如像iOS14.5之后，苹果的新政可能会使很多平台无法从技术端做调整，但因为我们刚开始搭建的模型就是机器学习，本身扩展性好，适应性也强。所以，不管是技术的变化还是政策的变化，Moloco都会有相应灵活的调整方案。”基于机器学习，Moloco还可以提供一站式的营销服务，针对不同客户在不同国家、不同的KPI来提供投放和服务。李子瞻表示，“我们提供给客户的不只是买量的服务，最主要是可以提供更加透明的数据分享。即通过Moloco的平台，客户可以看到所有广告投放相关的数据，并可以自己去调整预算。从流量的变化，素材的投放，到最后的分析等所有数据，客户都可以通过MolocoDSP可帮助市场营销人员快速扩大获客规模，并通过实践检验的预测模型实现更大的用户生命周期价值。Moloco聚焦中国移动应用出海的趋势，Moloco认为，中国企业出海的很多精品引领潮流，成为国外企业模仿的对象，中国元素正逐步受到世界市场认可。“最早期出海时，很多中国公司做了基于安卓系统的工具类App。随着共享经济的发展，中国的移动支付App也引领了全球的浪潮。随后中国的游戏App成为了整个数字经济里非常重要的一环，很多好的游戏，大部分是来自于中国企业。随着TikTok等App国际化布局的加快，中国企业在短视频、新电商领域又引领浪潮，逐渐成为了国外企业的模仿对象。”此外，中国企业出海范围增大，全球化路径多点开花。据李子瞻介绍，中国企业不断尝试推广的方式，也愿意尝试更多其他独立的广告平台。同时，中国企业出海业务整体规模持续扩大，从独立开发者到不同规模的企业纷纷加入，增长势头较猛。以游戏公司为例，在李子瞻看来，早期很多游戏公司完全依赖谷歌等平台，后来行业越来越“内卷”，游戏公司纷纷尝试不同类型的广告平台进行广告投放。目前，很多头部公司越来越依赖基于机器学习等技术的广告平台，以实现更好投放效果、更高ROI和高数据透明度的目标。“游戏公司对于移动互联网广告平台的认知程度会越来越高，现在懂游戏的人更懂营销了，懂营销的人也更懂游戏了，在沟通层面会很容易达成共识。”但另一方面，李子瞻也表示，中国企业在出海过程中依然面临一系列挑战。“比如在本土化方面，中国企业对当地的文化和受众，以及安全合规领域的了解都有待提升。如何找到靠谱的合作伙伴，并在全球各地统一部署，同时满足不同地区的品牌建设和投资，都是需要企业思考的。”基于此，Moloco建议，中国企业需要先了解自身的产品定位和商业模式，之后再结合要投放的市场做一些本土化的设计，比如游戏的人物角色需要结合本土化的文化以及受众群体做一些更改。同时，企业可以通过靠谱的合作伙伴来实现业务的增长。李子瞻强调，衡量“靠谱”的几个重要标准是，能否提供专业的咨询服务，做到更好的机器学习算法，以及是否更懂当地的市场，并能提供一站式服务。在帮助中国企业出海的同时，Moloco也需要不断适应中国市场。据悉，Moloco在中国的发展策略是全球最佳实践与中国本土化经验相结合。2021年进入中国，时间不算很早，但我们的信心来自于全球的实践经验。我们在世界各地都拥有丰富的成功案例，也有信心为中国市场提供更多的本土化咨询服务，帮助中国的企业出海。同时，我们愿意在中国从零开始培养一个团队，因为我们需要更多新鲜的血液把中国的团队变得更加可持续，灵活和年轻。”今年3月，Moloco宣布，将继续加大在中国市场的投入，通过扩大团队、提升本土化服务助力中国企业的出海营销战略。同时，Moloco将重点发力游戏赛道，并关注电子商务、在线旅游及其他周期性消费领域。李子瞻透露，目前Moloco全球45%的收入来自于非游戏业务。“虽然看上去游戏占比55%是比非游戏多一些，但从整个全球市场来看，这是一个非常健康的分配比例。中国现在正是发力非游戏的初期，未来在中国市场，我们游戏和非游戏的收入分配比例也会慢慢趋近于在全球市场的比例。”展望未来，李子瞻表示，“Moloco将更加专注于机器学习算法的优化，并帮助客户更好地理解我们的算法，以及更高效地投放。Moloco还将更加充分地了解全球各个市场，谷歌、苹果等在数字合规方面一直都会提出新的要求，我们也会告诉客户应如何去规避风险。此外，Moloco将继续坚持一站式营销平台的定位，通过数据帮助客户更省心省力地布局出海策略。”

标题：求解刘慈欣“三体世界”背后难题,力学专家用机器学习加算法给出10...
https://www.jfdaily.com/news/detail?id=499913
keywords  0 grammer  2 
get_summary
科幻作家刘慈欣在其科幻小说《三体》中虚构了一个“三体世界”，也向公众科普了牛顿1687年提出的这个著名“三体问题”。小说中，“三体人”生活的行星在一个由3颗恒星组成的三体系统中运行，他们的天空时常同时出现二个、三个太阳或者一个太阳也没有，导致“三体文明”不断毁灭与重生。“三体问题”正是历史上悬而未决的著名科学问题。近期，来自上海交通大学船舶海洋与建筑工程学院的廖世俊教授及其博士生杨宇，以及暨南大学副教授李晓明在国际杂志《新天文学（NewAstronomy）》发表论文，将“机器学习”与其发明的极高精度数值算法相结合，提出了求解“三体问题”周期轨道的路线图，在数量级层面大大提高了计算效率，为获得“三体问题”海量、精确的周期轨道铺平了道路。对于专业人士或业余爱好者，其相关机器学习程序和周期轨道可在GitHub免费下载。图说：利用机器学习寻找同一族三体系统不同星球质量的周期轨道。左下角红色区域为用传统方法获得的少数已知周期轨道。相同颜色表示同一次外插、机器学习可以找到的周期轨道之最大区域。横轴和纵轴上m1和m2为两个星球质量（m3自牛顿提出“三体问题”三百余年来，科学家仅仅发现“三体问题”的3类周期轨道。为什么“三体问题”周期轨道如此难找？1890年法国科学家庞加莱发现，三体系统的运动轨道对初始条件非常敏感，任何微小扰动都会迅速增长，造成轨道很大的偏离；他还发现，三体系统各个星球运动的轨道通常不是周期性的。这种轨迹对微小扰动的敏感性，1963年被美国科学家劳伦茨再次发现，并提出著名的“蝴蝶效应”：北美洲的一个飓风，很可能是几周前远在南美洲的一只蝴蝶扇了几下翅膀所引发的！这种敏感特性的发现标志着混沌动力学的诞生，它与量子力学、相对论被认为是20世纪最伟大的三大物理理论之一。“三体问题”的混沌性质，也正是科幻小说《三体》中“三体文明”理论上讲，正是因为“三体问题”本质上的混沌性，导致采用传统的数值方法很难在一个较长时域内获得三体系统的准确轨道。众所周知，任何数值计算都存在误差。但2006年劳伦茨发现，数值误差同样会导致混沌系统的轨迹迅速偏离：如果采用双精度数据编程，用计算机数值求解混沌动力系统，任何算法都不能获得收敛的混沌轨迹。因此，即便在人类计算机性能达到每秒100亿亿次量级的2013年，塞尔维亚科学家舒瓦科夫和什诺维奇采用传统数值方法也仅发现“三体问题”11类周期轨道。其杰出的工作是“三体问题”的一个重大突破，但也从侧面反映出求解“三体问题”周期轨道之艰难。各国科学家一直在探索“三体问题”的奥秘。2009年，上海交通大学廖世俊提出一个获得混沌动力系统收敛轨迹的策略——精准数值模拟（CleanSimulation)，简称CNS。CNS能将数值误差降到任意小，从而可获得混沌系统足够长时间内收敛的数值解，在理论上为准确获得“三体问题”的周期轨道铺平了道路。基于CNS，2017年廖世俊团队成功获得等质量的“三体问题”695类周期轨道；2018年廖世俊团队与上海交通大学物理和天文学院景益鹏院士合作，应用CNS进一步成功获得两个质量相等的三体系统1349类全新的周期轨道。解放日报·上观新闻记者了解到，获取任意不等质量的三体系统之周期轨道更为困难。2021年廖世俊与暨南大学李晓明等合作，以一个已知的、具有相同质量的三体系统周期轨道为基础，成功应用CNS获得该三体系统任意不等质量的135445个周期轨道，将“三体问题”周期轨道数量增加了几个数量级，证实了CNS求解任意质量“三体问题”周期轨道（特别是长周期轨道）的有效性。值得一提的是，这135445个周期轨道很多是稳定的，其质量范围与2019年诺贝尔物理学奖获得者麦耶（MichelQueloz）所发现的太阳系外第一个环绕类太阳恒星的行星相近。因此，这些周期轨道很有可能在宇宙中确实存在，今后有可能被观测到。今年，为了进一步大幅提高计算效率，廖世俊、李晓明、杨宇将CNS结合机器学习，即一种从数据中分析获得规律并对未知数据进行预测的算法，而提出获得“三体问题”周期轨道的路线图。按此路线图，从用传统方法获得的、很小质量范围内的周期轨道出发，基于机器学习和CNS一步步地获得更大质量范围内的精确周期轨道，直至找到该类周期轨道中所有不同质量的精确周期轨道。最后，对于存在周期轨道的质量区域内任意质量的“三体问题”，机器学习都能足够精确地预测其周期轨道之初始条件、周期。“三体问题”周期轨道的求解，证实了CNS求解复杂混沌问题的有效性和潜力。理论上，CNS可应用于N体问题周期轨道的求解以及湍流研究等，为星系演化、复杂湍流的精确数值模拟等提供一个全新的研究工具。

标题：模块化的机器学习系统就够了吗?
http://www.163.com/dy/article/HADN6NUV0552ZE3W.html
keywords  0 grammer  0 
get_summary
认知神经科学研究表明，大脑皮层以模块化的方式表示知识，不同模块之间进行通信，注意力机制进行内容选择，这也就是上述提到的模块化和注意力组合使用。在近期的研究中，有人提出，大脑中的这种通信方式可能对深度网络中的归纳偏置有意义。这些高级变量之间依赖关系的稀疏性，将知识分解为尽可能独立的可重组片段，使得学习更有效率。在这里看见、读懂和连接硬科技！我们聚焦光电芯片、人工智能、航空航天、新能源、智能汽车、生物医药、科创金融等行业，并依托于科技创新情报SaaS服务商智慧芽所拥有的独特科技情报数据优势，与读者一起看见技术趋势，读懂硬科技产业，连接创新未来。“硬科技”由智慧芽创新研究中心出品。特别声明：以上内容(如有图片或视频亦包括在内)为自媒体平台“网易号”用户上传并发布，本平台仅提供信息存储服务。

标题：景联文科技:机器学习AI数据集产品汇总(三)
https://baijiahao.baidu.com/s?id=1736222391210589627&wfr=spider&for=pc
keywords  0 grammer  2 
get_summary
随着科技的不断发展，人工智能场景化应用不断落地。现如今，实现人工智能的方式主要以机器学习和深度学习为主。在实际应用中，深度学习算法多采用有监督学习模式，对于人工智能基础数据有着强依赖性。只有依托于海量且优质的数据来提高算法的准确性，才能使机器学习的质量达到最优效果。购买现成的数据集将是个不错的选择。景联文科技还可以针对特定人群、特定场景、特定数据类型提供个性化的数据定制采集标注服务，我们将全力协助客户得到满意的数据服务。2012年，有科研背景以技术发展为导向的高新技术企业和AI基础数据服务企业。为全球数千家人工智能从业公司和高校科研机构提供AI数据采集、数据标注、数据集产品、标注平台定制开发、假指纹采集和指纹防伪算法服务。景联文始终践行“做全球AI行业客户的数据参谋”的企业使命，助力人工智能技术加速数字经济相关产业质量变革、动力变革与效率变革，赋能传统产业智能化转型升级。

标题：神经网络、深度学习和机器学习模型可视化工具——Netron
https://baijiahao.baidu.com/s?id=1736210278724568442&wfr=spider&for=pc
keywords  0 grammer  1 
get_summary
有时候我们写完深度学习模型后，想看看代码实现的模型和我们预期是否一致，但是没有一个好的工具。最近发现有一个软件Netron支持对一部分深度学习模型可视化，源码地址[1]。NetronLite、Caffe、Keras、Darknet、PaddlePaddle、ncnn、MNN、CoreLite、TNN、Barracuda、Tengine、CNTK、TensorFlow.js、Caffe2PyTorch、TensorFlow、TorchScript、OpenVINO、Torch、Vitis需要说明的是，想可视化Pytorch实现的模型，需要将模型导出为torchscript文件。如果不了解torchrscript，可以参考pytorch官方文档->torchscript[2]。代码实现如下：nn.Sequential(nn.Conv1d(in_channels=embedding_dim,那么这时我们就可以就我们的模型可视化了。当然对应的模块，还会显示该模块的输入与输出以及其他信息，如下图所示：之前也尝试查看bilstm+crf实现的模型，也是能够查看的。其他的模型我们可以根据自己的需要去试一试哦。是一个一站式的学术社区，这里为科研工作者提供海量的文献资源、好用的文献阅读及管理工具，旨在联结各个学术领域中志同道合的学者，以学术大数据和人工智能助力科研，打造专业的学术交流平台。

标题：Snowflake 正试图将机器学习带给普通人
https://new.qq.com/omn/20220620/20220620A02RX200.html
keywords  0 grammer  0 
get_summary
上，这家数据库公司发布了许多旨在促进机器学习采用的公告。其中最主要的是增强了对Python（编写许多这样的项目，我们希望为客户提供一种针对他们的数据运行这类模型的方法，无论是大规模还是以安全的方式。”尽管机器学习是一个已有数十年历史的概念，但直到最近几年，计算、存储、软件和其他技术的进步才为广泛采用铺平了道路。模型可以作为快餐客户开发基于机器学习的订购系统的通用基础。在这种情况下，客户不参与底层模型的任何训练和调整，但仍然可以获得该技术的所有好处。

标题：什么是Transformer机器学习模型?
https://baijiahao.baidu.com/s?id=1736125716717343601&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
机器学习模型已经成为深度学习和深度神经网络技术进步的主要亮点之一。它主要用于自然语言处理中的高级应用。谷歌正在使用它来增强其搜索引擎结果。OpenAI传统的前馈神经网络并非旨在跟踪序列数据并将每个输入映射到输出。它适用于诸如图像分类之类的任务，但在诸如文本之类的序列数据上却失败了。处理文本的机器学习模型不仅必须处理每个单词，还必须考虑单词如何按顺序排列并相互关联。而一个单词的含义可能会随着句子中出现在它们之前和之后的其他单词而改变。出现之前，递归神经网络(RNN)是自然语言处理的首选解决方案。当提供一个单词序列时，递归神经网络(RNN)将处理第一个单词，并将结果反馈到处理下一个单词的层。这使它能够跟踪整个句子，而不是单独处理每个单词。递归神经网络(RNN)的缺点限制了它们的用处。首先，它们的处理速度非常缓慢。由于它们必须按顺序处理数据，因此无法在训练和推理中利用并行计算硬件和图形处理单元(GPU)。其次，它们无法处理长序列的文本。随着递归神经网络(RNN)深入到文本摘录中，句子开头几个单词起到的效果逐渐减弱。当两个链接的词在文本中相距很远时，这个被称为“梯度消失”的问题就会出现。第三，它们只捕捉到一个单词和它之前的单词之间的关系。实际上，单词的含义取决于它们之前和之后的单词。长短时记忆(LSTM)网络是递归神经网络(RNN)的继任者，能够在一定程度上解决梯度消失问题，并且能够处理更大的文本序列。但是长短时记忆(LSTM)的训练速度甚至比递归神经网络(RNN)还要慢，并且仍然无法充分利用并行计算。他们仍然依赖于文本序列的串行处理。做出了两个关键贡献：首先，它们使并行处理整个序列成为可能，从而可以将顺序深度学习模型的速度和容量扩展到前所未有的速度。其次，它们引入了“注意机制”，可以在正向和反向的非常长的文本序列中跟踪单词之间的关系。尽管存在差异，但所有这些类型的模型都有一个共同点——他们学习表达。神经网络的工作是将一种类型的数据转换为另一种类型的数据。在训练期间，神经网络的隐藏层（位于输入和输出之间的层）以最能代表输入数据类型特征的方式调整其参数，并将其映射到输出。最初的被设计为用于机器翻译的序列到序列(seq2seq)模型（当然，序列到序列模型不限于翻译任务）。它由一个编码器模块组成，该模块将来自源语言的输入字符串压缩为一个向量，该向量表示单词及其相互之间的关系。解码器模块将编码向量转换为目标语言的文本字符串。Transformer。首先，文本通过“标记器”，将其分解为可以单独处理的字符块。标记化算法可以取决于应用程序。在大多数情况下，每个单词和标点符号大致算作一个标记。一些后缀和前缀算作单独的标记（例如，“ize”、“ly”和“pre”）。标记器生成一个数字列表，表示输入文本的标记ID。然后将标记转换为“单词嵌入”。单词嵌入是一种试图在多维空间中捕捉单词价值的向量。例如，“猫”和“狗”这两个词在某些维度上可能具有相似的值，因为它们都用于关于动物和宠物的句子中。然而，在区分猫科动物和犬科动物的其他维度上，“猫”比“狼”更接近“狮子”。同样，“巴黎”和“伦敦”可能彼此更加接近，因为它们都是城市。然而，“伦敦”更接近于“英格兰”，“巴黎”更接近于“法国”，这是因为在一个国家的区分维度上。而单词嵌入通常有数百个维度。接下来，输入被传递到第一个编码器块，它通过“注意层”对其进行处理。注意层试图捕捉句子中单词之间的关系。例如，考虑side)这个句子。在这里，模型必须将“it”与“cat”相关联，将“its”与“bottle”相关联。因此，它应该建立其他关联，例如“big”和“cat”或“crossed”和“cat”。否则，注意层接收表示单个单词值的单词嵌入列表，并生成表示单个单词及其相互关系的向量列表。注意层包含多个“attention注意层的输出被馈送到前馈神经网络，该网络将其转换为向量表示，并将其发送到下一个注意层。Transformers解码器模块的任务是将编码器的注意向量转换为输出数据(例如，输入文本的翻译版本)。在训练阶段，解码器可以访问编码器产生的注意向量和预期的结果(例如，翻译的字符串)。解码器使用相同的标记化、单词嵌入和注意机制来处理预期结果并创建注意向量。然后，它在编码器模块中传递该注意向量和注意层，从而在输入和输出值之间建立关系。在翻译应用程序中，这是源语言和目标语言中的单词相互映射的部分。与编码器模块一样，解码器注意向量通过前馈层传递。然后其结果被映射到一个非常大的向量池，即目标数据的大小(在翻译的情况下，这可以涉及数万个单词)。提供了非常大的配对示例语料库（例如，英语句子及其相应的法语翻译）。编码器模块接收并处理完整的输入字符串。然而，解码器接收到输出字符串的掩码版本（一次一个单词），并尝试建立编码的注意向量和预期结果之间的映射。编码器尝试预测下一个单词，并根据其输出与预期结果之间的差异进行更正。这种反馈使转换器能够修改编码器和解码器的参数，并逐渐在输入和输出语言之间创建正确的映射。应用都需要编码器和解码器模块。例如，大型语言模型的GPT系列使用解码器模块堆栈来生成文本。BERT是谷歌研究人员开发的通过获取大量未标记文本的语料库，可以屏蔽其中的一部分，并尝试预测缺失的部分来进行大部分训练。然后，它根据其预测接近或远离实际数据的程度调整其参数。通过不断地重复这个过程，BERT可以通过在少量标记示例上进行训练来针对下游任务进行微调，例如问答、文本摘要或情感分析。使用无监督和自我监督的预训练可以减少注释训练数据所需的工作量。和他们正在解锁的新应用程序还有更多，这超出了本文的范围。研究人员如今仍在寻找从Transformer中获得更多帮助的方法。与其他神经网络一样是一种统计模型，能够以巧妙而复杂的方式捕捉数据中的规律性。虽然它们不像人类那样“理解”语言，但是其发展仍然令人兴奋并且可以提供更多新事物。

标题：Python机器学习、深度学习库总结(内含大量示例,建议收藏)
https://new.qq.com/omn/20220620/20220620A09VYK00.html
keywords  0 grammer  1 
get_summary
目前，随着人工智能的大热，吸引了诸多行业对于人工智能的关注，同时也迎来了一波又一波的人工智能学习的热潮，虽然人工智能背后的原理并不能通过短短一文给予详细介绍，但是像所有学科一样，我们并不需要从头开始”造轮子“，可以通过使用丰富的人工智能框架来快速构建人工智能模型，从而入门人工智能的潮流。人工智能指的是一系列使机器能够像人类一样处理信息的技术；机器学习是利用计算机编程从历史数据中学习，对新数据进行预测的过程；神经网络是基于生物大脑结构和特征的机器学习的计算机模型；深度学习是机器学习的一个子集，它处理大量的非结构化数据，如人类的语音、文本和图像。因此，这些概念在层次上是相互依存的，人工智能是最广泛的术语，而深度学习是最具体的：库有一个初步的了解，以选择能够满足自己需求的库进行学习，对目前较为常见的人工智能库进行简要全面的介绍。的一个扩展程序库，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库，底层使用编写，数组中直接存储对象，而不是存储对象指针，所以其运算效率远高于码。能够给用户提供良好的基于机器学习的生物信息图像分析服务，利用机器学习算法，轻松地分割，分类，跟踪和计数细胞或其他实验数据。大多数操作都是交互式的，并不需要机器学习专业知识。可以参考https://www.ilastik.org/documentation/basics/installation.html进行安装使用。编程语言的免费软件机器学习库。它具有各种分类，回归和聚类算法，包括支持向量机，随机森林，梯度提升，k均值和库定义了许多数学物理的特殊函数，包括椭圆函数、贝塞尔函数、伽马函数、贝塔函数、超几何函数、抛物线圆柱函数等等。)提供了易于使用的接口，以及一套用于分类、分词、词干、标记、解析和语义推理的文本处理库、工业级自然语言处理NLP。它可以用于构建处理大量文本的应用程序；也可以用来构建信息提取或自然语言理解系统，或者对文本进行预处理以进行深度学习。拥有漂亮直观的交互式用户界面，非常适合新手进行探索性数据分析和可视化展示；同时高级用户也可以将其作为的模块化机器学习库。它的目标是为机器学习任务和各种预定义的环境提供灵活、易于使用且强大的算法来测试和比较算法。SVMS、K-NN、随机森林以及决策树中使用监督分类法，它还可执行特征选择，可以形成不同的例如无监督学习、密切关系传播和由是一款设计为效率和灵活性的深度学习框架。它允许混合符号编程和命令式编程，从而最大限度提高效率和生产力。以百度多年的深度学习技术研究和业务应用为基础，集深度学习核心训练和推理框架、基础模型库、端到端开发套件、丰富的工具组件于一体。是中国首个自主研发、功能完备、开源开放的产业级深度学习平台。是一个深度学习工具包，通过有向图将神经网络描述为一系列计算步骤。在这个有向图中，叶节点表示输入值或网络参数，而其他节点表示对其输入的矩阵运算。

标题：免费!吴恩达机器学习新课程又来了!
https://www.163.com/dy/article/HA7VJ5B40519EA27.html
keywords  0 grammer  2 
get_summary
据吴恩达介绍，为了让ML初学者能更好地学习理解，在每堂课开始，授课教师都会对神经网络、深度学习、决策树等机器学习的概念进行直观详尽的讲解。不过，如果想要更好地参与课堂和实验、并希望有人为你批改作业的话，每月需支付49美元（约合人民币328元）。https://www.coursera.org/specializations/machine-learning-introductionhttps://twitter.com/AndrewYNg/status/1537119911889145858特别声明：以上内容(如有图片或视频亦包括在内)为自媒体平台“网易号”用户上传并发布，本平台仅提供信息存储服务。

标题：【美企项目实践】苹果(北美)-机器学习工程师项目
https://new.qq.com/omn/20220620/20220620A01RQO00.html
keywords  0 grammer  4 
get_summary
应用Tensorflow建立神经网络模型，并利用相关API进行数据处理，特征提取，从而进行目标识别及人脸识别收获苹果（北美）项目实战经验，解锁软件工程师必备技能，完成基于Tensorflow及CNN的人脸识别项目

标题：与机器学习相比,深度学习有什么优点?
https://baijiahao.baidu.com/s?id=1735983881804759956&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
从这个问题上我看到了学术界一个非常重要的潜规则，名字取的好才是第一生产力，似乎一个好名字不仅利于推广，还能给算法提点呢。为什么我想谈这一点呢，因为这个问题问的有点不太严谨，感觉提问者对这两个概念都没理解的很清楚。深度学习是机器学习的子集，这怎么比较呢。非要比那可以问相对于其他机器学习算法，深度学习有什么优势。什么是深度学习？深度学习是机器学习的子集，机器学习是人工智能的子集。人工智能其实已经经历好几波浪潮，概率学派统计学派贝叶斯学派你方唱罢我登场也已经战了几个来回了。但从Alexnet开始，CNN异军突起。之后，Resnet带头冲锋，这一波新的人工智能浪潮正式打响。中间还有像AlphaGo，GAN，Attention这样的大事件，也是把这把火越少越热。在这波浪潮在缘起的时候，大家关心比较多的就是网络深度，比如resnet就是以残差连接这一大杀器将网络层数大幅加深。之后densenet直接在命名以网络深度为卖点。最近，在网络层数方面，transform架构的DeepNet已经有方法把网络加至1000层。因此，深度学习最大的优势是足够深，网络容量足够大。这一点能带来很多的好处，最明显的好处就是足够深的网络能容纳更丰富的语义信息。这样的网络只要学得动，没有过拟合，那么表现出来得性能就更像一个智能，而不是个智障。深度学习的优点还有很多。其实自从alexne开始，新的算法大部分都属于深度学习领域了，如果说新算法比老算法有什么优势，零零散散可以说一大堆，所以，就着这个不太严谨的问题，我谈谈刚刚想到的一些想法。

标题：AI大牛推特热议:机器学习中最美/最优雅的idea是什么?
https://baijiahao.baidu.com/s?id=1735141184910465265&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
之所以会有这样的疑问，是因为他发现数学家和物理学家们就经常谈论美学，机器学习领域却很少这样，也很好奇为什么。所谓梯度下降法，就是一种寻找目标函数最小化的方法，它利用梯度信息，经过不断迭代调整参数来寻找合适的目标值。假设当你站在山上时雾很大，想尽快下山的你却无法看清下山路线，那么就只能利用周围的环境信息走一步算一步，也就是以当前位置为准，找到最陡峭的地方往下走。重复这个计算过程，就能达到山谷。我们在求解机器学习算法的模型参数时，为了让所得模型可以更好地捕捉到数据中蕴含的规律，进行更准确地预测，一般会最小化损失函数得到参数估计值。，一直致力于对人工神经网络进行逆向工程的工作，曾先后就职于OpenAI和谷歌大脑，现在是一家主攻大型模型安全性的初创公司的联合创始人。他还举证称，“我的一位朋友用3行随机梯度下降法就可以替代复杂的传统方法解决凸问题（SVM、CRF）”。小哥解释说，自己一开始也认为机器学习的美体现在复杂的数学和巧妙的证明上，但后来才渐渐发现不是这样的，他给出了以下理由和具体例子来支撑他的观点。首先在他看来，机器学习中的很多理论应用到神经网络中都可以“发现”非常漂亮的图像，比如用梯度下降得到的分组卷积图。顺理成章，他将梯度下降法比作生物学中的进化，认为它们都是通过简单的过程就能产生具有高度复杂性的东西。与此同时也有网友反驳道，就拿反向传播机制来说，我们的大脑根本都不存在这个东西，怎么能说机器学习和生物学很像呢？“我觉得梯度下降还是一个数学问题，和进化无关；并且我还得说一句，数学之美远超进化和生物学之美，更别提梯度下降比进化聪明了几个数量级呢。”有人进一步回怼。，可能也存在表述不准确的原因。但无论如何，这些结论都不涉及解释人工神经网络的生物学合理性。除此之外，一切都是他的直觉感受，大家随意接受和反驳就好。“毕竟在得到最终结果之前，你得经历各种报错和bug的折磨。要我说，我还是觉得物理学中的诺特定理是最优雅的东西。”即使机器学习到了2022年，你也不能在没法确保能达到一个有趣结果的情况下就随意“鼓捣”宝贵的GPU，这样的话，谁还在乎机器学习到底优不优雅呢？不过总而言之，不少人都表示DeepMind研究员提出的这个问题非常好，大家的评论也都很有意思，值得一读。

标题：原创Reddit宣布收购前Facebook工程师创立的Spell机器学习平台
https://it.sohu.com/a/559175544_121360237
keywords  0 grammer  0 
get_summary
在官网上写道，其使命是为任何希望使用机器学习（ML）和人工智智能（AI）人们，打造强大、可靠、安全的最佳平台。

标题：双相障碍共病边缘型人格障碍的诊断要点:来自机器学习方法的新发现
https://news.medlive.cn/psy/info-progress/show-189418_60.html
keywords  0 grammer  0 
get_summary
共病边缘型人格障碍（BP/BDP）与纯双相障碍（BP）的准确率达到79.6%，而鉴别双相障碍共病边缘型人格障碍（BP/BDP）与纯边缘型人格障碍（BPD）的准确率为61.7%。双相障碍（BP）与边缘型人格障碍（BPD）的鉴别有一定难度。一般认为，尽管两者均可表现为显著的情绪不稳，但双相障碍较边缘型人格障碍呈现出更强的发作性特征，且边缘型人格障碍患者在身份、人际关系等方面存在其他一系列突出的特点，可供鉴别。然而，BP共病BPD的情况也不鲜见；此类共病如何与单纯的BP或BPD鉴别，同样值得探讨。在这一背景下，澳大利亚新南威尔士大学黑狗研究院AdamBayes等使用机器学习手段，探索了BP共病BPD（BP/BPD）与纯BP或纯BPD的鉴别要点。本项研究本月在线发表于2021年，同一研究团队使用机器学习手段探讨了纯BP与纯BPD的鉴别诊断，并实现了较高的准确率（PMID型78人，平均年龄33岁）、52名纯BPD患者（平均年龄30岁）及53名BP/BPD患者（平均年龄32岁），后两组女性比例较高。这些受试者完成了四种自评工具的评估，分别为人格测量问卷（PM，113个条目），情绪调节量表（DERS，36个条目），认知情绪调节问卷（CERQ，36个条目），以及父母类型问卷（MOPS，30个条目），其中一些量表条目对于鉴别共病与未共病患者可能尤其有价值。研究者使用技术手段对这些因素进行了探索，并评估了基于这些因素的算法鉴别共病与非共病患者的准确性。具体研究设计详见原始文献。44）；女性；“难以控制冲动行为”（DERS_Impulse）；“其他人如果了解真实的我，就会不喜欢我”（PM鉴别BP/BPD和纯BPD时，最重要的因素是“难以参与目标导向行为”（DERS_Goals），随后为“对情绪缺乏觉察”（DERS_Aware）；双相障碍家族史阳性；“缺乏有效的情绪管理策略”（(DERS_Strategies）。其他有价值的因素包括“我在有压力的时候容易生气和失去冷静”（PM41）、来自父亲的虐待史（MOPS_Father_Abuse）、“难以控制冲动行为”（DERS_Impulse）。BPD。压力相关偏执及其他BPD的核心特征有助于鉴别BP/BPD与纯BP，而情绪调节困难有助于鉴别BP/BPD与纯BPD。此外，冲动和愤怒对于鉴别BP/BPD(本网站所有内容，凡注明来源为“医脉通”，版权均归医脉通所有，未经授权，任何媒体、网站或个人不得转载，否则将追究法律责任，授权转载时须注明“来源：医脉通”。本网注明来源为其他媒体的内容为转载，转载仅作观点分享，版权归原作者所有，如有侵犯版权，请及时联系我们。)

标题：趋势分析:物联网中的人工智能、机器学习和边缘计算
https://baijiahao.baidu.com/s?id=1736012845724359651&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
“边缘”有一些神奇之处。例如，在环境科学中，有人研究“栖息地边界”，即某些种类的植物大量生长在边缘，但没有进一步生长。同样，在天文学中，我们也看到了发生在宇宙边缘的戏剧性现象。这似乎与人类社会没有什么不同，因为一场新的革命正在酝酿，高计算能力正在向边缘移动，逐渐被称为边缘计算。物联网的核心包括收集和分析数据、洞察力以及涉及机器、人员、事物和地点的流程的自动化。因此，物联网是传感器、执行器、连接性、云和边缘计算和存储、人工智能和机器学习智能以及安全的组合。工业4.0，也被称为第四次工业革命，严重依赖物联网技术，重新定义汽车、交通、医疗、公共出行、零售和商业。由于AI和ML推动了所有5G和物联网创新，它们的结合是一种自然现象。随着数据存储和计算成为“结构连续体”。这种结构涉及所有领域，大量AI、ML和边缘计算解决方案最终将根据用例找到它们的最佳区域和发挥的作用。、Amazon和Microsoft，引入了全新规模的IaaS(基础设施即服务)，具有可管理性、透明性和分析能力，且价格具有竞争力。来自LoRAWAN、NB-IoT和快速增长的5G的众多选项使得提供先进的物联网解决方案更容易、更可行。其他一些转变也开始显现，更明显地体现在玩家参与度方面。例如，电信运营商的超大规模(如上所述)和运营技术转型(OT，如机器控制系统进入IT领域)。越来越多的物联网应用正在构思和构建，希望它们将被企业和消费者消费和支付。然而，由于各种原因，它最好是从离客户最近的位置交付，而该区域是活动的中心。这个空间本质上意味着，工作负载将从中央数据中心转移到本地或本地专用边缘计算云。中央数据中心的转移有时可能意味着将本地数据中心转移到本地专用的边缘计算云。边缘云是智能边缘设备(包括传感器、节点和网关)与软件(算法、安全栈、连接模块、传感和执行组件、完整栈中的处理器)的简单组合，以处理每个网关中的数百个传感器。数以千计的此类节点通过高级集群和节点重新配置自动聚集在一起，在某种超级智能物联网网络出现故障时重新路由。各种技术已经就位，可扩展的用例也越来越普遍。他们甚至制定了标准化的协议，将数据转移到云上进行非关键数据处理，但实时数据处理仍保留在边缘云上。随着芯片产量的增加和价格的下降，我们预计数百万的物联网设备将通过边缘云进行通信，并帮助人类生活。AI/ML向边缘的移动是一个不可逆的过程，实际上也是一种必然。AI图像处理、物体检测和音视频识别能力也得到了显著提高。它们现在作为附加功能提供给某些芯片供应商。我们预计，随着部署的增加，这些技术将变得更加普遍，价格也将得到改善。目前尚不清楚边缘云市场的规模是否会像它所宣称的那样大。有证据表明，这可能是一个高增长的行业。但是，此服务的所有权元素仍在定义中;电信公司与超大规模企业、专业的企业系统集成商以及云边缘专家。边缘云是一种万能的解决方案，而不是特定于用例的解决方案。目前还不清楚哪些应用程序需要边缘云功能，可能是用于制造RPA和医疗保健，与零售应用程序的应用程序。企业仍在评估商业案例，模型的成本将随着时间的推移而增加。边缘计算是寻找应用程序的解决方案吗?几微秒的往返延迟是否使消费者准备花更多的钱?我们是否将边缘存储误认为边缘计算，并将所有这些应用程序一视同仁?集中式数据中心的沉没成本如何?它们现在会未被充分利用，还是最终只存储非关键数据?如果边缘计算的一切都是关于实时的，那么5G切片可能是解决方案的一个重要元素。如果是这样的话，电信公司和超大规模企业之间能否建立起一种可持续的商业模式。这些真相很快就会大白于天下。随着该领域的进一步发展，AI/ML技术将如何在新的世界秩序中重新组合将变得更加明显。这可能会定义服务提供者的新定位，他们与解决方案提供商的联盟，以及组合的商业模型。

标题：吴恩达预热新课!万字博客回顾机器学习算法起源
https://baijiahao.baidu.com/s?id=1735228792459093611&wfr=spider&for=pc
keywords  0 grammer  3 
get_summary
吴恩达最近发布了一篇博客，介绍了几个基础算法的起源和直观理解，还宣传了一下自己重置版机器学习课程：目的竟是为了复习一遍！最近吴恩达在博客网站上发表了一篇特刊，表示自己由于常年使用神经网络，已经快忘了该怎么用传统的机器学习算法了！因为深度学习并非在所有场景下都好用，所以在「盲目」使用神经网络受挫后，痛定思痛，写了一篇文章，为一些传统的机器学习算法提供一些直观上的解释。Specialization课程提供一些基础知识，包括线性回归、逻辑回归、梯度下降、神经网络、决策树、K-means聚类等算法。因为我已经有一段时间没有使用提升决策树了，我直觉上认为它们需要的计算量比实际要多，但选择神经网络是一个错误的决定。这次经历给我上了一课，告诉我学习和不断刷新基础知识的重要性。如果我重新熟悉了提升树，我就会做出更好的决定。机器学习和其他技术领域一样，随着研究人员社区在彼此工作的基础上不断发展，一些贡献经过时间的考验后，长盛不衰，并成为进一步发展的基础。从住房价格预测器到文本、图像生成器，一切新算法都建立在基础算法上（例如线性和逻辑回归、决策树等）和基础概念（如正则化、优化损失函数、偏差/方差等）的核心思想上。坚实的、时刻更新的基础知识是成为一名高效的机器学习工程师的一个关键。许多团队在日常工作中都会用到这些想法，而博客文章和研究论文也常常假定你对这些思想很熟悉，这些常识基础对于我们近年来看到的机器学习的快速进步至关重要。我的团队花了很多时间来讨论最核心的教学概念，为各种主题制定了广泛的教学大纲，并在其中设计了课程单元的原型。我希望最后的结果是一套易于理解的课程，能够帮助任何人掌握当今机器学习中最重要的算法和概念，包括深度学习，但也包括很多其他东西，并能够建立有效的学习系统。本着这种精神，我们决定探讨一些领域内最重要的算法，解释了它们是如何工作的，并介绍它们不为人知的起源。regression）可能是机器学习中的最重要的统计方法，至于谁发明了这个算法，一直争论了200年，仍未解决。Legendre）在预测一颗彗星的位置时，发表了将一条线拟合到一组点上的方法。天体导航是当时全球商业中最有价值的科学，就像今天的人工智能一样。Gauss）坚持认为，他自1795年以来一直在使用这种方法，但他认为这种方法太过琐碎，无法写出来。高斯的说法促使Legendre发表了一份匿名的附录，指出「一位非常有名的几何学家毫不犹豫地采用了这种方法」。一辆汽车的油耗y和它的重量x之间的关系取决于直线的斜率w（油耗随重量上升的陡峭程度）和偏置项b（零重量时的油耗）：y=w*x+b。在训练期间，给定汽车的重量，算法预测预期的燃料消耗。它比较了预期和实际的燃料消耗。然后通过最小二乘法，使平方差最小化，从而修正w和b的值。考虑到汽车的阻力，有可能产生更精确的预测。额外的变量将直线延伸到一个平面。通过这种方式，线性回归可以接收任何数量的变量/维度作为输入。线性回归算法在当年可以帮助航海家追踪星星，后来帮助生物学家（特别是查尔斯-达尔文的表弟弗朗西斯-高尔顿）识别植物和动物的遗传性状，进一步的发展释放了线性回归的潜力。1922年，英国统计学家罗纳德-费舍尔和卡尔-皮尔逊展示了线性回归如何融入相关和分布的一般统计框架，再次扩大了其适用范围。当然，数据从来没有被完美地测量过，而且多个变量之间也存在不同的重要程度，这些事实也刺激了线性回归产生了更复杂的变体。例如，带正则化的线性回归（也称为岭回归）鼓励线性回归模型不要过多地依赖任何一个变量，或者说要均匀地依赖最重要的变量。如果你要追求简化，使用L1的正则化就是lasso回归，最终的系数更稀疏。换句话说，它学会了选择具有高预测能力的变量，而忽略了其他的变量。神经网络中最常见的一种神经元就是线性回归模型，往往后面再跟着一个非线性激活函数，所以线性回归是深度学习的基本构件。Logistic回归将logistic函数拟合到数据集上，以预测在某一事件（例如，摄入马钱子）发生特定结果（例如，过早死亡）的概率。2、将中心向右或向左调整意味着需要更多或更少的毒药来杀死普通人。陡峭的坡度意味着确定性：在中间点之前，大多数人都能活下来；超过中间点，那就说得再见了。一个平缓的斜率更宽容：低于曲线的中间点，超过一半的人可以存活；更远的地方，不到一半。3、设置一个阈值，比如说0.5，曲线就成了一个分类器。只要把剂量输入模型，你就会知道你应该计划一个聚会还是一个葬礼。Logistic函数可以描述多种多样的现象，并具有相当的准确性，因此Logistic回归在许多情况下提供了可用的基线预测。在医学上，它可以估计死亡率和疾病的风险；在政治中，它可以预测选举的赢家和输家；在经济学中，它可以预测商业前景。想象一下，在黄昏过后的山区徒步旅行，你会发现除了你的脚以外看不到什么。而你的手机没电了，所以你无法使用GPS应用程序来寻找回家的路。训练网络的一种方法是，通过反复计算实际输出和期望输出之间的差异，然后改变网络的参数值来缩小该差异，从而使损失最小化，或其输出中的误差最小。网络的参数值相当于景观上的一个位置，而损失是当前的高度。随着你的下降，你提高了网络的能力，以计算出接近所需的输出。使用梯度下降，你也有可能被困在一个由多个山谷（局部最小值）、山峰（局部最大值）、马鞍（马鞍点）和高原组成的非凸形景观中。事实上，像图像识别、文本生成和语音识别这样的任务都是非凸的，而且已经出现了许多梯度下降的变体来处理这种情况。K-means的聚类就是基于这种先验想法，将数据点分为多个group，无论这些group是通过人类机构还是其他力量形成的，这种算法都会找到它们之间的关联。Lloyd）是贝尔实验室标志性创新工厂和发明原子弹的曼哈顿项目的校友，他在1957年首次提出了k-means聚类，以分配数字信号中的信息，不过他直到1982年才发表。K-means聚类首先会寻找group的中心，然后将数据点分配到志同道合的group内。考虑到数据量在空间里的位置和要组成的小组数量，k-means聚类可以将与会者分成规模大致相同的小组，每个小组都聚集在一个中心点或中心点周围。在训练过程中，算法最初需要随机选择k个人来指定k个中心点，其中k必须手动选择，而且找到一个最佳的k值并不容易。对于每个聚类簇，它计算所有被分配到该组的人的平均位置，并将平均位置指定为新的中心点。每个新的中心点可能都不是由一个具体的人占据的。在计算出新的中心点后，算法将所有人重新分配到离他们最近的中心点。然后计算新的中心点，调整集群，以此类推，直到中心点（以及它们周围的群体）不再移动。K-means算法的原始形式仍然在多个领域很有用，特别是因为作为一种无监督的算法，它不需要收集潜在的昂贵的标记数据，它的运行速度也越来越快。例如，包括scikit-learn在内的机器学习库都得益于2002年增加的kd-trees，它能极快地分割高维数据。

标题：机器学习与房地产销售价格指数,能擦出什么火花?
https://baijiahao.baidu.com/s?id=1735963709025193975&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
6月16日，国家统计局公布了：2022年5月份，70个大中城市商品住宅销售价格变动情况，包含新建住宅、二手住宅的同比、环比价格变化，为判断房地产市场的运行情况提供了详实的参考数据。从新建住宅价格的环比增长率来看，无论是整体的数据，还是分建筑面积的数据，都呈现相似的态势：负增长的城市多于正增长的城市，位于-0.52%-0%这个区间的城市数量最多，一定程度上能代表整体的行情。从统计值来看，尤其要关注的是50%分位数，它能较好的反映房价变化的平均水平。本月，无论是整体，还是按面积细分，环比增长率的50%分位数都是负值，也就是说对大多数城市而言，新建住宅价格较上月在下跌。对于新建住宅价格的同比增长率，下跌的也占多数，但是分化更加严重。具体来说，大幅下跌、大幅上涨的城市数量都比环比要多，处于温和变化的反而在减少。进一步分析统计值，50%的分位数线比环比低得多，也就是说房价的同比价格下跌较为剧烈。再来看标准差的情况，明显和环比数据不在一个层次上，波动幅度较大。二手住宅价格的环比增长率和新建住宅类似，下跌的多于上涨的，但是显著不同的是：二手住宅价格下跌在-0.52%之外的多于新建住宅，上涨的少于新建住宅。再来分析同比增长率，处于下跌区间的占多数，而且有一个很显著的特征：增长率在-1.03%到-0.52%这个区间的城市，与增长率在-0.52%到0%这个区间的城市相差无几，这是一个新的变化。通过特征工程和数据降维，得到每个城市住宅价格指数的二维特征数据，经过聚类处理后，得到如上图所示的A、B、C三类城市。受限于数据和技术，聚类结果有一定误差。A类城市有18个，这类城市算得上房地产市场中的“优等生”，其新建住宅、二手住宅的价格波动幅度较小，没有明显的大起大落，行情较为稳定。但是同比情况的分化较为严重。B类城市有41个，能代表当前的整体行情。这类城市的住宅价格大多处于下行态势，且同比下行幅度还不小，城市间的分化也较为严重。C类城市有11个，这些城市的情况较为严峻，住宅价格下跌的情况较为普遍，尤其是同比数据，50%分位数、标准差都不太好看。

标题：日本理化所利用量子化学机器学习,荧光分子合成成功率75%
https://baijiahao.baidu.com/s?id=1735072228941172521&wfr=spider&for=pc
keywords  0 grammer  3 
get_summary
近日，日本理化学研究所(RIKEN)化学家展示了一种新型分子设计策略，通过结合机器学习和量子化学方法，制造了六种荧光化合物。新的分子设计方法预测了8种化合物会发出荧光，其中6个在紫外线照射下会发出荧光(图中显示了5个)，包括一种以前没有报道过的化合物(未在图中显示)，图片来自日本理化学研究所(RIKEN)在荧光分子实验中，前述方法合成目标分子成功率达75%，有望大幅节省在实验室中制造和测试化合物的时间。传统的分子设计方法是先从一个性能接近理想的分子开始，然后通过实验，反复试错来改进分子。这样的方法不仅耗时，并且存在偶然性，无法保证最终得到的分子就是最佳分子。长期以来，化学家一直想要扭转前述情况。他们希望能够从期望的分子特性出发，搜索所有可能的分子，并找到符合要求的分子。但可获得的数据中，仅包含所有分子的极小一部分。Sumita和同事展示了一种新的策略，使搜索全部分子成为可能，并且不需要单独制造每种化合物。相关成果近日发表在《科学进展》(Science前述团队使用了一种全新的分子发生器，其利用机器学习来根据所需特性，给出可用分子的建议。然后，通过执行量子化学计算的模拟器来预测分子特性，并在指定计算时间内循环重复这些步骤，以找到符合条件的分子。荧光分子生成的工作流程，图片来自论文为了证明前述方法是否有效，研究小组通过该方法来寻找能发出肉眼可见波长荧光的分子。经过五天的数字运算，计算机得出了3600多个候选分子。团队挑选了其中八种进行合成，发现其中六种是荧光的，其中包括一种从未报道过的荧光化合物。“这是首次将新分子发生器与量子化学计算相结合，以用于发现荧光分子。”日本理化学研究所化学家MasatoSumita说道，“我对这种方法的高成功率感到非常惊讶。当我们在实验室进行分子合成时，8个候选分子中有75%发出了荧光。”Sumita，图片来自日本理化学研究所(RIKEN)寻找荧光分子是对前述方法的严格测试。因为与光吸收等更简单的分子性质不同，荧光是一种光致发光的冷发光现象，包含多步骤的过程，因此很难从分子结构上进行预测。未来，前述团队打算将机器学习和量子化学相结合的新方法，应用到研究其他化学性质上，并试图用其同时优化多种化学特性。

标题：统计精进:基于实例介绍分类预测常用的统计与机器学习方法
https://baijiahao.baidu.com/s?id=1735465165152638956&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
下面的例子是慕尼黑大学机器学习入门讲座的一部分内容。该项目的目标是为手头的问题创建并比较一个或几个机器学习管道，同时进行探索性分析并对结果进行阐述。名患者的数据。根据病人是否有肝病，观察结果被分为两类。除了我们的目标变量外，还提供了十个主要是数字的特征。为了更详细地描述这些特征，下表列出了数据集中的变量。显然，一些测量值是其它变量的一部分。例如，血清总胆红素是直接胆红素和间接胆红素水平的总和；而白蛋白的数量则用于计算血清总蛋白以及白蛋白-球蛋白比率的数值。因此，一些特征是彼此高度相关的，下面会进行处理。可以看到，目标变量（即肝病与非肝病患者）的分布是相当不平衡的，如柱状图所示：有肝病和无肝病的患者数量分别为416和167。一个类别的代表性不足，可能会使ML模型的性能恶化。为了研究这个问题，作者还在一个数据集上拟合了模型，在这个数据集上，随机地对少数人类别进行了过度抽样，结果是一个完全平衡的数据集。此外，我们还应用了分层抽样，以确保在交叉验证过程中保持各类的比例。唯一的离散特征可以看到，一些指标特征是极度右偏的，包含几个极端值。为了减少离群值的影响，并且由于一些模型假设了特征的正态性，我们对这些变量进行了"类中，男性的比例略高，但总体而言，差异不大。除此之外，正如我们之前提到的，在两个类别中都可以观察到性别"类中的分散性更大，正如箱线图的长度所示。总的来说，这些特征似乎与目标相关，所以将它们用于这项任务并建立它们与目标的关系模型是有意义的。正如我们在数据描述中提到的，有些特征是由另一个特征间接测量的。这表明它们是高度相关的。我们要比较的一些模型假设是可以看到，其中四对有非常高的相关系数。看一下这些特征，很明显它们是相互影响的。由于模型的复杂性应该最小化，并且由于多重共线性的考虑，我们决定每对特征中只取一个。在决定保留哪些特征时，我们选择了那些关于肝病的更具体和相关的特征。因此，我们选择了白蛋白，而不是白蛋白和球蛋白的比例，也不是蛋白质的总量。同样的观点也适用于使用直接胆红素的量而不是总胆红素。关于天门冬氨酸转氨酶和丙氨酸转氨酶，我们没有注意到这两个特征的数据有任何根本性的差异，所以我们任意选择了天冬氨酸转氨酶。，这对k-NN模型尤其重要。下表显示了最终的数据集和我们应用的转换。注意：与对数或其他转换不同，缩放取决于数据本身。在数据被分割之前对数据进行缩放会，因为训练集和测试集的信息是共享的。由于数据泄露会导致更高的性能，缩放应该总是单独应用于ML工作流程所引起的每个数据分割。因此，我们强烈建议在这种情况下使用，其中包含最终的数据集和一些元信息。此外，我们还需要指定正类，因为软件包默认将第一个正类作为正类。正类的指定对后面的评估有影响。为了找到最佳的超参数，我们使用随机搜索来更好地覆盖超参数空间。我们定义了要调整的超参数。我们只调整了table(po_over$train(list(task_liver))$output$truth())在定义了学习器、选择了嵌套重采样的内部方法和设置了调整器之后，我们开始选择外部重采样方法。我们选择了分层的5倍交叉验证法，以保持目标变量的分布，不受过度采样的影响。然而，事实证明，没有分层的正常交叉验证法也会产生非常相似的结果。为了对不同的学习器进行排名，并最终决定哪一个最适合手头的任务，我们使用了**基准测试(benchmarking)**。下面的代码块执行了我们对所有学习者的基准测试。为80%和20%。此外，性能指标的选择对于不同学习器的排名至关重要。虽然每一个都有其特定的使用情况，但我们选择了从上面的结果可以看出，无论是否应用了超采样，逻辑回归、LDA、QDA和NB在训练和测试数据上的表现非常相似。另一方面，k-NN、CART和随机森林在训练数据上的预测效果要好得多，这表明过度拟合。事实证明，在没有超采样的情况下，逻辑回归、LDA、k-NN、CART和随机森林在敏感性方面得分很高，而在特异性方面得分相当低；另一方面，QDA和天真贝叶斯在特异性方面得分相对较高，但在敏感性方面却没有那么高。根据定义，高灵敏度（特异性）源于低的假阴性（阳性）率，这在数据中也有体现。关于哪种学习器效果最好，也包括是否应该使用超量取样，在很大程度上取决于敏感性和特异性的现实意义。就实际的重要性而言，两者中的一个可能会超过另一个很多倍。想想典型的HIV快速诊断测试的例子，以低特异性为代价的高灵敏度可能会引起（不必要的）震惊，但除此之外并不危险，而低灵敏度则是非常危险的。正如通常的情况一样，这里不存在黑白分明的"最佳模型"。回顾一下，即使有超额取样，我们的模型没有一个在灵敏度和特异性方面表现良好。在我们的案例中，我们需要思考：以低灵敏度为代价的高特异性的后果是什么，这意味着告诉许多肝病患者他们是健康的；而以低特异性为代价的高灵敏度的后果是什么，这意味着告诉许多健康患者他们有肝病。在没有进一步的特定主题信息的情况下，我们只能说明在所选择的特定性能指标上表现最好的学习器。如上所述，基于然而，我们进行的分析决不是详尽的。在特征层面上，虽然我们在分析过程中几乎只关注了机器学习和统计分析方面，但也可以更深入地挖掘实际的主题（肝病），并尝试更彻底地理解变量以及潜在的相关性和互动性。这可能也意味着要再次考虑已经删除的变量。此外，可以对数据集进行特征工程和数据预处理，例如使用主成分分析。关于超参数的调整，可以考虑使用更大的超参数空间和评估数量的不同超参数。此外，调整也可以应用于那些被我们标记为基线学习者的一些学习器。最后，还有更多的分类器存在，特别是梯度提升和支持向量机可以另外应用于这项任务，并有可能产生更好的结果。）(https://mlr3gallery.mlr-org.com/posts/2020-09-11-liver-patient-classification/)轻松实现Kaplan-Meier（K-M）生存曲线、中位生存时间（95%可信区间）、Logrank检验及HR值

标题：机器学习模型发现新冠病毒新变种,AI精准预测哪种毒株来势汹汹
https://baijiahao.baidu.com/s?id=1734761612638537820&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
论文地址：https://www.science.org/doi/10.1126/science.abm1208#con13研究人员使用流感数据共享全球倡议数据库，截止到2022年1月，共600多万个新冠病毒基因组，训练了机器学习模型。PyR0一种分层贝叶斯回归模型，对公开获得的全部病毒基因组进行可扩展分析，并可应用于任何病毒基因组数据集。https://news.sciencenet.cn/htmlnews/2022/5/479928.shtm

标题：谷歌浏览器将使用机器学习来阻止弹出窗口
https://baijiahao.baidu.com/s?id=1735573866685796676&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
谷歌浏览器宣布将很快集成一项新功能，该功能将自动阻止烦人的网站提示和弹出窗口。谷歌在一篇博文中强调，下一个版本的Chrome网络浏览器将采用机器学习模型，该模型将自动阻止来自网站的权限提示和通知。此功能背后的想法是确保您在浏览网站时获得无缝体验。该公司强调，机器学习预测将完全在您的设备上进行，不会向公司发送任何数据。

标题：机器学习非线性回归预测:混凝土强度
https://baijiahao.baidu.com/s?id=1735335098758374761&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
/kaggle/input/concrete-compressive-strength/Concrete/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921:/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921:

标题：机器学习实战:基于Scikit-Learn、Keras和TensorFlow第2版PDF
https://baijiahao.baidu.com/s?id=1735610119958646439&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
《机器学习实战：基于Scikit-Learn、Keras和TensorFlow第2版》中文PDF+英文PDF+源代码文件分享。2和Scikit-Learn的新版本进行了全面更新，通过具体的示例、非常少的理论和可用于生产环境的Python框架，从零帮助你直观地理解并掌握构建智能系统所需要的概念和工具。全书分为两部分。第一部分介绍机器学习基础，涵盖以下主题：什么是机器学习，它试图解决什么问题，以及系统的主要类别和基本概念；第二部分介绍神经网络和深度学习，涵盖以下主题：什么是神经网络以及它们有什么用，使用TensorFlow和Keras构建和训练神经网络的技术，以及如何使用强化学习构建可以通过反复试错，学习好的策略的代理程序。第一部分主要基于Scikit-Learn，而第二部分则使用TensorFlow和Keras。通过本书，你会学到一系列可以快速使用的技术。每章的练习可以帮助你应用所学的知识，你只需要有一些编程经验。所有代码都可以在GitHub上获得。Géron是机器学习方面的顾问。曾就职于Google，在2013年到2016年领导过YouTube视频分类团队。Wifirst公司的创始人并于2002年至2012年担任该公司的首席技术官。于2001年创办Ployconseil

标题：谷歌在机器学习中证明量子计算优越性
https://baijiahao.baidu.com/s?id=1735485006850202819&wfr=spider&for=pc
keywords  0 grammer  2 
get_summary
量子技术有望彻底改变我们获取和处理实验数据以了解物理世界的方式。理论上，将数据从物理系统转换到稳定的量子存储器并使用量子计算机处理数据的实验装置，与使用经典计算机测量物理系统并处理结果的传统实验相比，具有显著的优势。然而，由于当今量子计算机容易出错，并且没有足够多的量子比特来进行纠错，因此尚未实现这种优势或优越性（advantage）。他们通过对多达40个超导量子比特和1300个量子门进行实验，证明使用当今NISQ量子处理器可以实现实质性的量子优势，这种优势是指数级的，并且实现优势所需的量子资源是适度的。具体来说，量子系统可以从比传统实验所需数量少得多的实验中学习。提出并分析了三类具有指数量子优势的学习任务——预测物理系统的属性、执行量子主成分分析、学习物理动力学。(a)谷歌“悬铃木”处理器的布局。总共有53个超导transmon量子比特；(b)用于学习状态和学习一维动力学的布局。研究人员将40个量子比特划分为20个系统量子比特和20个存储量子比特。要么制备备好系统量子比特的未知状态，要么对系统量子比特应用一个未知过程；(c)用于学习二维动力学的布局。“人们对使用量子技术提高我们学习能力的潜力感到非常兴奋，”论文第一作者、加州理工学院的理论物理学家和计算机科学家Hsin-Yuan（a）量子实验与传统实验对比。量子/传统实验与运行量子/经典学习算法的量子/经典机器对接，可以存储和处理量子/经典信息。（b）学习物理状态。每个实验产生一个物理状态，在传统设置中，测量每个以获得经典数据（测量可能取决于先前的测量结果），并将数据存储在经典存储器中；在量子增强设置中，可以相干地改变存储在量子存储器中的量子信息（用颜色的变化来说明）。有了足够大的量子存储器，量子机器可以简单地存储每个的副本。经过多轮实验，在量子存储器上进行量子处理，然后进行测量。（c）学习的物理过程ε。每个实验都是在ε下的演化，在传统设置中，经典机器使用经典位串指定输入状态给ε，并获得经典测量数据；在量子设置中，进化ε相干地改变了量子机器的存储器：ε的输入状态与量子机器中的量子存储器纠缠，输出态被量子机器相干地检索出来。为什么量子计算机的性能要好得多？研究人员说，一个关键步骤是存储两个被检查系统的副本，然后将它们纠缠在一起。这种方法只有在量子硬件上才有可能[2]。第一个实验描述了涉及任意数量项目的量子系统的某些属性，例如具有n个量子比特的量子计算机。使用量子计算机，可以使用量子比特本身的镜像量子态，并根据需要复制和操纵它；在经典计算机能够可靠识别属性之前，需要进行重复测量。相比之下，量子计算机可以将系统的副本存储在其存储器中，从而允许对其进行重复复制和处理。实验表明，这些问题可以在量子计算机上多项式时间解决，其中量子比特的数量被提高到恒定的幂（表示为nk)。相比之下，使用经典硬件的时间缩放为一个常数：随着量子比特数量的增加，经典硬件所需的时间增加得更快。第二个任务是量子主成分分析，其中计算机用于识别对量子系统行为影响最大的性质。选择这一点的部分原因是这种分析被认为对当今量子处理器中的错误引入的噪声相对不敏感。该团队从数学上表明，在经典系统上重复测量以进行分析所需的次数随着量子比特的数量呈指数级增长。使用量子系统，分析可以用恒定的重复次数来完成。最后一种情况是允许物理过程影响量子系统的状态，使其进化到新的状态。目标是找到一个可以准确预测新状态的过程模型。同样，使用经典系统意味着获得足够测量值的挑战随着量子比特的数量呈指数级增长，但在使用量子计算时增长得更慢。学习物理状态的量子优势。(a)基于量子增强实验的监督机器学习（ML）模型。N个重复的量子增强实验被执行，数据被送入一个门控递归神经网络（GRU）。GRU中的神经元被聚集起来预测输出。(b)有监督的ML模型的训练过程。通过对小系统规模（<8）的无噪声模拟，来训练有监督的ML模型，以确定在未知状态下，两个-量子比特Pauli算子的期望值哪个更大。研究团队以交叉熵为训练损失；然后使用有监督的ML模型，利用在“悬铃木”处理器上运行的噪声量子增强实验的数据对较大的系统规模（8≤≤20）进行预测。将预测正确的概率视为预测精度，随机猜测产生的预测精度为0.5。(c)达到≥70%准确率所需的实验数量的量子优势。这里（Q）对应的是运行基于量子增强实验的监督ML模型的结果，（C）对应的是运行传统策略的结果。虚线是任何常规策略（C,LB）的下限，即使在一个有噪声的量子处理器上运行，量子增强实验也大大超过了理论上可实现的最佳常规结果（C,学习物理动力学的量子优势。(a)无监督的机器学习（ML）模型。我们对每个物理过程εk进行500次重复的量子增强实验（每次访问ε两次），并将数据送入无监督的ML模型（PCA），学习描述不同物理动力学的一维表示ε1,ε2,...。同样，也考虑对每个物理过程ε的1000次重复的已知最佳常规实验（每次访问ε一次）获得的数据应用无监督的ML。(b)通过无监督的ML学习的一维动态的表示。每个点对应于一个不同的物理过程ε。底部的垂直线显示了每个ε的精确一维表示。一半的过程满足时间反转对称性（蓝色菱形），而另一半则不满足（红色圆圈）。当输入量子增强实验的数据时，ML模型准确地发现了潜在的对称性模式。相比之下，当输入传统实验的数据时，ML模型未能做到这一点。(c)通过无监督的ML学习到的二维动力学的表示。(d)在“悬铃木”处理器上实现的几何图形。研究团队考虑了两类不同的连通性几何，以实现一维（顶部）和二维（底部）动力学。研究团队实验分析了量子技术如何提高发现自然界中新现象的能力。通过超导量子处理器中进行的多达40个量子比特的实验表明，在使用当今含噪声中等规模量子（NISQ）平台时，大量的量子优势已经很明显。以前对量子优势的许多研究都集中在已知输入的计算任务上，而这一新的工作则集中在学习任务上，目标是学习一个先验的未知物理系统。结果表明，监督和无监督的机器学习模型采用从量子增强实验中获得的数据，可以预测物理系统的属性，并发现常规实验范围之外的潜在结构。可以设想，未来的量子传感系统将能够把检测到的量子数据传输到量子存储器，然后用量子计算机处理存储的数据。由于目前缺乏合适的先进传感器和换能器，团队仅进行了概念验证实验，尽管如此，它们也已经验证强大的量子优势突出推进了量子平台的潜力。[1]https://www.science.org/doi/10.1126/science.abn7293[2]https://arstechnica.com/science/2022/06/quantum-computer-succeeds-where-a-classical-algorithm-fails/

标题：为什么TensorFlow可以做机器学习开发?
https://baijiahao.baidu.com/s?id=1735216079682232056&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
TensorFlow）简化了获取数据、训练模型、提供预测和改进未来结果的过程，实现机器学习远没有以前那么令人生畏。将大量机器学习和深度学习模型和算法（也称为神经网络）捆绑在一起，并通过常见的编程隐喻使它们变得有用。它使用等框架竞争，可以训练和运行深度神经网络，用于手写数字分类、图像识别、词嵌入、循环神经网络、用于机器翻译的序列到序列模型、自然语言处理和基于允许开发人员创建数据流图——描述数据如何通过图或一系列处理节点移动的结构。图中的每个节点都代表一个数学运算，节点之间的每个连接或边都是一个多维数据数组，或称张量。行代码中定义，同样的训练代码只需要几行代码。但如果你想“揭开面纱”，做更细粒度的工作，比如编写自己的训练循环，你可以这样做。为机器学习开发提供的最大好处是抽象。开发人员可以专注于整体应用程序逻辑，而不是处理实现算法的细节，或者找出将一个函数的输出连接到另一个函数的输入的正确方法。TensorFlow应用程序的开发人员提供了更多便利。每个图形操作都可以单独且透明地进行评估和修改，而不是将整个图形构建为单个不透明对象并立即对其进行评估。这种所谓的“急切执行模式”作为旧版实现的一些细节使得某些训练作业很难获得完全确定的模型训练结果。有时，在一个系统上训练的模型与在另一个系统上训练的模型会略有不同，即使它们提供了完全相同的数据。这种差异的原因很棘手——一个原因是随机数是如何播种的以及在哪里播种；另一个与使用分支有一个选项，可以通过几行代码在整个工作流程中启用确定性。但是，此功能以性能为代价，并且仅应在调试工作流时使用。有许多其他相似之处：引擎盖下的硬件加速组件、允许即用即设计工作的高度交互的开发模型，以及已经包含许多有用的组件。对于需要在短时间内启动并运行的项目的快速开发，PyTorch通常是更好的选择，但API——Python、C++、Scala、R、JavaScript、Julia、Perl、Go——尽管它的原生

标题：机器学习中的回归分析,你了解多少?
https://baijiahao.baidu.com/s?id=1735137291956021946&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
回归分析是一种统计技术，用于确定目标变量（例如房屋价格）和输入参数（例如位置、建筑面积、便利设施等）之间的关系。回归技术本质上有助于训练机器学习模型，以查找历史数据中存在的数学模式/趋势。此后，当出现新的数据点时，模型会尝试根据其以前的数据学习来预测结果。工程师可以使用多种回归技术进行预测。根据用例、业务假设、数据集的分布等选择特定类型的回归分析。下面我将介绍一些必须知道的回归分析类型。逻辑回归是一种用于解决分类问题的回归方法。与目标/因变量是连续的其他回归技术不同，这里的因变量是离散的（在多项式回归中，方程中的每个元素都可以取任意“k”值，而在多元线性回归中，一个元素可以取多个变量，但必须具有相同的次数（“k值”）。

标题：融入机器学习,让Chrome浏览器更“懂”你
https://baijiahao.baidu.com/s?id=1735237739264707765&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
还记得曾经网上搜索时，点开一个链接，广告页面就占满屏幕，随后桌面上就蹦出一个个来路不明的软件。曾经随意点开Chrome浏览器也不例外。这并不足为奇，不过最近谷歌在其功能中融入了机器学习，似乎有些不一样的改变。M92更新时就有了，当时Google就已经将此功能的性能进行了改进，将计算网络钓鱼分类结果的时间从1.8秒减少到100毫秒，速度有了进一步提升。在今年三月份，谷歌更新了机器学习的模型，在识别数量上也有一定的提升，能够识别网络钓鱼攻击或试图提供恶意下载的网站是之前的2.5倍。有些无聊的通知请求弹出时，即使你每次都点的是关闭或者不允许。但是它还是会“不厌其烦”的出现，显得有些“不识趣”了。谷歌提到：“为了减少用户在浏览网页时所受到的干扰，Chrome浏览器会预测权限提示不太可能被接受的时间，并将这些提示设置为静音。在下一个版本的Chrome中，我们将推出一个ML（Machine另外，一般来说，在浏览器中的一些功能选项的位置都是相对来说比较固定的，但根据你的使用习惯和其他的预测数据，在你可能使用的时间和位置提供不同的选项按钮，是不是很神奇呢？（当然，也有可能并不是你想要的）在未来的版本中，谷歌可能会提供这一功能，实时地调整Chrome工具栏，比如分享图标或语音搜索就能够进行转换（如下图）。不过，如果用户还是习惯于使用手动转换按钮，也是可以的。有的网友会质疑这样的调整会不会带来新的困扰，比如没有在想用的时候显现出选项。但有的网友似乎有些其他方面的担忧：这是个可怕的想法。我希望谷歌是根据不同年龄的人来测试他们的UI而不是单单只根据20多岁的人来进行测试。对于很多60多岁的人来说，不停变化的UI元素简直是噩梦。事实上，很多年龄稍大的人甚至都不能够分辨UI的标志。他们是依靠肌肉记忆和位置来使用的。一个有趣的例子是关于我的70多岁的奶奶，她根本看不懂来电横幅，除非来电显示占满整个屏幕。所以我给她买了一部iPhone，但是后来更新系统到iOS是的，如果谷歌引入“AI”功能，那么随机显示/隐藏内容来就会惹恼最终用户。这和你是20岁还是70岁无关，随机变化的UI元素，让你不完全知道为什么图标会出现或消失，这只会和用户站在对立面。

标题：当前计算机专业是否只有机器学习和算法这条路,编程还有出路吗
https://baijiahao.baidu.com/s?id=1734937111717909858&wfr=spider&for=pc
keywords  0 grammer  3 
get_summary
首先，计算机大类专业目前确实更注重大数据和人工智能相关知识，而机器学习则是一个比较重要的切入点，所以很多同学都会在本科阶段重视机器学习、深度学习相关知识的学习。一方面机器学习是大数据分析的两种基本方式之一，另一方面机器学习也是人工智能的六大研究方向之一，而且被称为是初学者打开人工智能大门的钥匙，因此在当前大数据、人工智能的时代背景下，学习机器学习可以认为是顺应技术发展趋势的选择。其次，学习机器学习与学习软件开发并不冲突，因为学习机器学习也需要具有一定的编程基础，而且很多机器学习的实践活动，包括科研实践和项目实践活动，都是需要与行业场景相结合的，这就需要一个完整的解决方案，所以在学习机器学习的过程中，同样能够提升自己的编程能力，未来也可以走软件开发路线。实际上，计算机专业有很多研究生同学在读研期间就是主攻机器学习、深度学习方向的，但是这其中更多的同学还是会拿开发岗的offer。虽然近两年随着算法岗开始逐渐向业务算法岗倾斜，拿到算法岗offer的同学有所增加，但是毕竟算法岗的整体竞争比较激烈，很多同学为了进大厂也会放弃一些中小厂的算法岗。总体上来说，计算机大类专业的同学，不论是本科生还是研究生，我个人的建议是要立足开发来寻求发展，也就是说要重视编程能力的提升，如果一味专注于机器学习和算法而忽略了编程能力的提升，未来在发展空间上会受到一定的限制。对于编程感兴趣的同学来说，同样不能忽略机器学习和算法知识的学习，虽然现在的编程已经不能仅仅说是算法问题了，但是在大数据、人工智能的时代背景下，给自己奠定一个扎实的算法基础还是有必要的，而且当前不少大厂的开发岗面试也会考察算法知识。

标题：机器学习中耳朵能听出茧的特征值特征向量,是这数学关系式
https://baijiahao.baidu.com/s?id=1735438073986962016&wfr=spider&for=pc
keywords  -2 grammer  0 
get_summary
每个行当都有每个行当的专业术语，如果初来乍到，想涉足特定行业，而不懂规矩连术语都不懂的话，估计百分之九十九的概率是混不下去的。机器学习深度学习当然也不能免俗，其中的基本概念常识比如特征值特征向量，不了解的话，根本就举步维艰。又是一个简简单单的公式，又是一个需要我们牢记的概念。上面这个表述很数学，也很基础。如果这个都不好理解，那可真要恶补一下数学知识了。当然机智客这里的话又说回来了，数学中的定义，单纯看，未必能看出什么东西。跟很多其他数学知识一样，机智客当时学的时候也是，单独拎出来似乎能理解，放到数学中就不大懂了。每个中国汉字都认识，组合起来就不理解了。数学就是如此神奇。。嗯，瞧这数学，一层压一层，一环套一环的。上次学的矩阵的秩在这里用到了吧。后续的展开就不具体聊了，机智客很多的文章涉及到的知识点只是在平时工作或应用中遇到的知识点，现学现卖，学习并整理出来。朋友如果需要可以自行翻看，如果还要延伸更多，则可以自己找专业书籍和资料补充。

标题：Chrome 103将完全利用浏览器中的机器学习技术来封锁网站请求
https://baijiahao.baidu.com/s?id=1735235937130451426&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
会不会对于所访问的网站一直跳出通知感到很烦？Google于周四（6/9）宣布，下一版的Chrome将利用机器学习（MachineLearning，ML）技术模型来预测用户对通知的反应，并且悄悄地将它们封锁。虽然Google并未公布版本别，但从发布的进程来看，该功能应该会出现在预计于6月下旬出炉的ChromeGoogle很早就开始于服务中采用ML技术，例如用来过滤垃圾邮件或更新地图，而在Chrome上也不例外，像是以ML技术替图片自动产生图说，或是替线上视频加上即时字幕等，最近一次的应用则是在替用户阻挡恶意网站的SafeBrowsing激活新的ML模型以来，它所识别的恶意及网络钓鱼网站比先前的模型多了2.5倍，进一步改善了用户的浏览安全。新的ML应用则期望改善浏览经验，着重于用户与网页通知之间的交互。Google认为，网页通知一方面可传送网站更新给用户，但另一方面不断跳出通知也令用户不堪其扰，先前的做法是根据用户过去与类似通知的交互，来判断是否要堵住不受欢迎的通知，但当时必须将资料发送至Google进行分析，新版Chrome则将利用ML模型，直接于设备上执行预测。因此，未来当新的ML模型判断特定通知可能会被用户忽略时，它会直接将通知静音并封锁，只是会在网址列上秀出该资讯，让用户依然可激活该通知。在不久的将来，Google更会让ML模型依照用户的作息与习惯，自动调整Chrome浏览器上的工具栏，例如当用户经常在某个时刻进行语音输入时，届时语音输入的图标就会出现在第一顺位。总之，Google企图发挥ML模型的最大潜力以替Chrome用户带来更个性化的服务。

标题：6月16日直播预约,PWmat—MLFF机器学习在分子动力学中的应用
https://baijiahao.baidu.com/s?id=1735033867247370044&wfr=spider&for=pc
keywords  0 grammer  2 
get_summary
北京龙讯旷腾科技有限公司是成立于2015年的国家高新技术企业，是国内材料计算模拟工具软件研发创新的领导者，致力于开发满足“工业4.0”所需的原子精度材料研发Q-CAD（quantum-computerdesign）软件。公司自主开发的量子材料计算软件PWmat（平面波赝势方法并基于GPU加速）可以进行电子结构计算和从头算分子动力学模拟，适用于晶体、缺陷体系、半导体体系、金属体系、纳米体系、量子点、团簇和分子体系等。

标题：机器学习平台——3个被数据分析师夸爆的Python编程小功能!
https://baijiahao.baidu.com/s?id=1735046759333620741&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
说起高考，近几年越来越多的考生选择报考计算机专业，即使没能报考相关专业，也会积极去学习相关知识和编程技能，比如最近在大学生中，就掀起了“Python学习热”。的确，作为目前应用最为广泛的编程语言之一，在随着数据挖掘分析在企业业务层面的深度应用，我们会发现，在实际业务场景中，数据分析工作已经逐渐分为2个层次；较为流程化但与业务深度结合的数据分析工作，会更倾向于让业务人员通过专业分析工具进行自主探索分析，比如我们的Tempo大数据分析平台，就可以通过拖拽式、智能化的简便操作，帮助业务人员快速上手数据分析工作，快速将业务数据价值变现。而针对一些更加细致、专业化的建模分析需求，其实还是通过代码的方式去实现会更加高效灵活。Tempo大数据分析平台正是考虑到广大专业数据分析人员的实际需求，专门开发了【扩展编程】这一功能模块，让自定义编程和平台中已有的节点结合使用，快速提高数据分析工作效率！为了让广大数据分析师在Tempo平台中，既可以通过编程实现更加灵活的建模，也能避免原生Python编程的一些使用局限，我们还通过广泛的用户调研，在【扩展编程】模块设计了3个提高Python编程易用性的小功能，下面我们一起来看看这三个功能都具体解决了哪些问题~我们常说“一图胜千言”，对于数据分析工作来说更是如此。分析结果最终还是要直接赋能实际业务，由于Python代码行的形式让分析结果可读性极低，分析人员无法快速实现对数据的直观探索分析，导致整体工作效率的降低。调试和分析是Python开发中非常重要的环节，但代码校验报错结果难查看的问题，让很多数据分析人员大伤脑筋。尤其是现在很多企业数据分析项目是在自己的数据平台中进行的，分析人员只能把平台中的代码复制到第三方平台中进行校验，调试好后再粘贴回平台中运行，非常麻烦。在一般的企业数据分析项目中，预先有可能会设置有多套Python环境，版本并不统一。这就会导致分析人员常常并不清楚自己当前使用的到底是哪一套Python环境，在编码时引用的Python包，在不同的Python环境上运行很有可能会出现缺失，引起代码执行报错，而且这种错误信息并不好排查，一旦报错只能从头再来，让程序员们白白做了无用功。以往专业数据分析人员遇到以上问题，只能选择用更多的时间和精力去修复bug，那么在我们的Tempo大数据分析平台之中，又是如何解决这些问题，让Python编程的应用更简便、更高效、更适应企业实际需求呢？针对传统编码数据分析方式难以直观展示数据分析结果的缺点，Tempo大数据分析平台——扩展编程模块特别支持在代码区域设置通过matplotlib/seabron等图形方法实现节点洞察，校验通过后，就可以直接在洞察区域中查看绘制的图形，比如：折线图、直方图、条形图、饼图等。Tempo大数据分析平台—扩展编程模块中的控制台功能，可以直接在指定区域中显示错误信息和代码中需要print的部分，便于使用者快速发现问题，立即调整代码。除了帮助专业数据分析人员提高工作效率，控制台中的【示例】小模块还内置了常用的参考代码，可以帮助一些不太能熟练应用Python编程语言的小白用户，通过复用或小部分修改逻辑代码的方式，也能快速完成数据逻辑处理工作。用好这个功能，团队内部的技能培训也能更有章法了呢！Python编程最怕丢包缺包问题，在Tempo大数据分析平台中，我们内置了Python环境版本和包版本的信息查询组件，分析人员在编码之前，可以预先查询一下当前的Python环境版本和包版本，如果发现有缺包就可以在编码工作开始之前补充安装，把“丢包缺包”造成的代码报错风险降到最低。今天的Tempo小课堂中，小T主要给大家介绍了如何通过Python扩展编程的三个小功能，帮助代码偏好的专业数据分析人员减少不必要的麻烦操作，提高工作效率。

标题：机器学习中降维,让人觉得跟科幻片里空间折叠降维打击一样
https://baijiahao.baidu.com/s?id=1734351349921311921&wfr=spider&for=pc
keywords  0 grammer  1 
get_summary
我们时不时会在现代商业社会里看到降维打击这个词。而它则是真实存在于机器学习等学科中的。所以降维（数据降维）不局限于机器学习，在其他领域也有。毕竟越底层的知识越通用，就跟计算机技术类似。数学，是横跨所有理工科，是各个领域知识的基础。因此降维我等大众凡人常从科幻小说电影或现代商业社会中听到的概念，的确是数学等学科上或者机器学习里的一个很重要的知识点。因为它，用来对数据压缩和提取，以便利于我们计算和观摩，这个过程，真的有点类似科幻影视作品里的三维等多维空间折叠。瞧，原本是学科技术方面的词，被科幻和商业社会借了兜售给我们，然后我们学习时溯源又看到这些概念，可谓人生就是一个圈，兜兜转转又返还。就不说商业概念或科幻小说洗脑了，即便单纯从名字看，我们也能明白降维的字面意义：降低维度。影视作品里，常常是从高纬度向低纬度的投影折叠，从而碾压低纬度。其实在学科概念里，它的深层次意义是降低随机变量的个数，有效信息的提取综合及无用信息的摈弃。毕竟对于我们生活在三维空间（你带上时间说四维也可以）里的普通凡夫俗子而言，想象比我们更高的维度空间，的确有困难。想象都有困难，更别提再模拟或计算了。所以对于高维数据，成千上万的维造成的维度灾难，我们的幼小的人类智商撞见了不吓死也得残疾。因此需要降维，一方面方便于我们人类大脑来理解和分析，另一方面，也方便于电脑来计算和分析。没错，电脑再比人脑快，依然难以面对大规模的高维数据。将数据降维到我们能理解和计算的程度，有效提取了有用信息，同时还排除了很多噪声无用数据，可谓一举多得，这样才利于我们可视化分析了。其实呢，降维就是从高维数据到低维数据的映射，就是一个函数映射。而这个映射函数具体一点，在机器学习中也分好多种，比如按使用样本的标签值分类成有监督降维和无监督降维，比如线性降维和非线性降维。这里不细说了。机智客觉得对于我们而言，容易直观接受的就是常见的数据降维方法是从高维空间投影到低纬子空间，也就是投影方法（另外还有一个流形学习）。而我们在学习数学或者看技术教科书的时候，就会经常看到类似某某是某某在某个维度上的投影。和这个差不多，虽然刚开始学时，乍一听觉得云里雾里的，学多了就懂了。

标题：「Day01」新手机器学习入门好文,强烈推荐
https://baijiahao.baidu.com/s?id=1729202668088833493&wfr=spider&for=pc
keywords  0 grammer  1 
get_summary
这些问题正是本文要回答的。同时，在人工智能火热的今天，本文是为了帮助新手同学快速了解，做一个学习的分享，倘若某天和朋友聊起了人工智能机器学习，也好作些准备传统上如果我们想让计算机工作，我们给它一串指令，然后它遵照这个指令一步步执行下去。有因有果，非常明确。但这样的方式在机器学习中行不通。机器学习根本不接受你输入的指令，相反，它接受你输入的数据!“统计”思想将在你学习“机器学习”相关理念时无时无刻不伴随，相关而不是因果的概念将是支撑机器学习能够工作的核心概念。我相信大家都有跟别人相约，然后等人的经历。现实中不是每个人都那么守时的，于是当你碰到一些爱迟到的人，你的时间不可避免的是要浪费。我就碰到过这样的一个例子。对我的一个朋友小Y而言，他就不是那么守时，最常见的表现是他经常迟到。当有一次我跟他约好3点钟在某个麦当劳见面时，在我出门的那一刻我突然想到一个问题：我现在出发合适么？我会不会又到了地点后，花上30分钟去等他？我决定采取一个策略解决这个问题：我搜寻能够解决这个问题的知识。但很遗憾，没有人会把如何等人这个问题作为知识传授，因此我不可能找到已有的知识能够解决这个问题。：我去询问他人获得解决这个问题的能力。但是同样的，这个问题没有人能够解答，因为可能没人碰上跟我一样的情况。第：我问自己的内心，我有否设立过什么准则去面对这个问题？例如，无论别人如何，我都会守时到达。但我不是个死板的人，我没有设立过这样的规则事实上，我相信有种方法比以上三种都合适。我把过往跟小Y相约的经历在脑海中重现一下，看看跟他相约的次数中，迟到占了多大的比例。如果这个值超出了我心里的某个界限，那我选择等一会再出发。假设我跟小Y约过5次，他迟到的次数是1次，那么他按时到的比例为80%，我心中的阈值为70%，我认为这次小Y应该不会迟到，因此我按时出门。如果小Y在5次迟到的次数中占了4次，也就是他按时到达的比例为20%，由于这个值低于我的阈值，因此我选择推迟出门的时间。这个方法从它的利用层面来看，又称为经验法。在经验法的思考过程中，我事实上利用了以往所有相约的数据。因此也可以称之为依据数据做的判断。，也就是用来预测小Y是否迟到的量。假设我把时间作为自变量，譬如我发现小Y所有迟到的日子基本都是星期五，而在非星期五情况下他基本不迟到。于是我可以如果把我们的自变量再增加一个。例如小Y迟到的部分情况时是在他开车过来的时候(你可以理解为他开车水平较臭，或者路较堵)。于是我可以关联考虑这些信息。建立一个更复杂的模型，这个模型包含两个自变量与一个因变量。如果我希望能够预测小Y迟到的具体时间，我可以把他每次迟到的时间跟雨量的大小以及前面考虑的自变量统一建立一个模型。于是我的模型可以预测值，例如他大概会迟到几分钟。这样可以帮助我更好的规划我出门的时间。在这样的情况下，决策树就无法很好地支撑了，如果我把这些建立模型的过程交给电脑。比如把所有的自变量和因变量输入，然后让计算机帮我生成一个模型，同时让计算机根据我当前的情况，给出我是否需要迟出门，需要迟几分钟的建议。那么计算机执行这些辅助决策的过程就是机器学习的过程。机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(迟到的规律)，并利用此模型预测未来(是否迟到)的一种方法。通过上面的分析，可以看出机器学习与人类思考的经验过程是类似的，不过它能考虑更多的情况，执行更加复杂的计算。事实上，机器学习的一个主要目的就是把人类思考归纳经验的过程转化为计算机通过对数据的处理计算得出模型的过程。经过计算机得出的模型能够以近似于人的方式解决很多灵活复杂的问题。从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，很显然，我希望获得房价与面积的某种规律。那么我该如何获得这个规律？用报纸上的房价平均数据么？还是参考别人面积相似的？无论哪种，似乎都并不是太靠谱。这组数据中包含了大大小小房子的面积与价格，如果我能从这组数据中找出面积与价格的规律，那么我就可以得出房子的价格。125万。这个结果与我前面所列的100万，120万，140万都不一样。由于这条直线综合考虑了大部分的情况，因此从“统计”意义上来说，这是一个最合理的预测。，例如抛物线，那么拟合出的就是抛物线方程。机器学习有众多算法，一些强力算法可以拟合出复杂的非线性模型，用来反映一些不是直线所能表达的情况。，我的模型就越能够考虑到越多的情况，由此对于新情况的预测效果可能就越好。这是机器学习界“数据为王”思想的一个体现。一般来说(不是绝对)，数据越多，最后机器学习生成的模型预测的效果越好。通过我拟合直线的过程，我们可以对机器学习过程做一个完整的回顾。首先，我们需要在计算机中存储历史的数据。接着，我们将这些数据通过机器学习算法进行处理，这个过程在机器学习中叫做“训练”，处理的结果可以被我们用来对新的数据进行预测，这个结果一般称之为“模型”。对新数据人类定期地对这些经验进行“归纳”，获得了生活的“规律”。当人类遇到未知的问题或者需要对未来进行“推测”的时候，人类使用这些“规律”，对未知问题与未来进行“推测”，从而指导自己的生活和工作。通过这样的对应，我们可以发现，机器学习的思想并不复杂，仅仅是对人类在生活中学习成长的一个模拟。由于机器学习不是基于编程形成的结果，因此它的处理过程不是因果的逻辑，而是通过归纳思想得出的相关性结论。”。通过学习历史，我们从历史中归纳出人生与国家的规律，从而指导我们的下一步工作，这是具有莫大价值的。当代一些人忽视了历史的本来价值，而是把其作为一种宣扬功绩的手段，这其实是对历史真实价值的一种误用。其实，机器学习跟模式识别，统计学习，数据挖掘，计算机视觉，语音识别，自然语言处理等领域有着很深的联系。，形成了计算机视觉、语音识别、自然语言处理等交叉学科。因此，一般说数据挖掘时，可以等同于说机器学习。同时，我们平常所说的机器学习应用，应该是通用的，不仅仅局限在结构化数据，还有图像，音频等应用。在这篇内容对机器学习这些相关领域的介绍有助于我们理清机器学习的应用场景与研究范围，更好的理解后面的算法与应用层次。可以看出机器学习在众多领域的外延和应用。机器学习技术的发展促使了很多智能领域的进步，改善着我们的生活。

标题：AI论技|图机器学习——从入门到入门
https://baijiahao.baidu.com/s?id=1728706646820961453&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
ML)是一类基于机器学习的处理图域信息的方法，机器学习领域关于图机器学习的研究热情日益高涨，图机器学习已经成为各大深度神经网络顶会的研究热点。图面呈现的各种图元及块都是矢量数据。若使用目前最为成熟的基于位图的CNN-based图像识别算法，需要将这些矢量图转化成位图，这个过程无疑会丢失关键的坐标信息，损失数据精度，并且大大提升数据体积。在大规模的的图纸场景中，这种做法越发难以为继。因此，直接基于矢量图像数据出发进行图纸空间的特征提取与语义识别成为自动出图需要解决的关键问题。矢量图像数据可以被抽象成为一种图结构。近年来图机器学习的研究突破使得我们在筑绘通生产环境中大规模使用图机器学习处理非结构化数据时的出色能力，使其在网络数据分析、推荐系统、物理建模、自然语言处理和图上的组合优化问题方面都取得了新的突破。由于其较好的性能和可解释性，图机器学习图是描述和分析具有交互关系的实体的通用语言。这意味着我们不是将世界或给定域视为一组孤立的数据点，而是根据这些实体之间的网络和关系来真正的考虑它。我们认为这些实体是具有相互关联的关系的，可以通过图的结构来体现这些关系。同时，很多类型的数据可以自然地表示为图结构。例如，计算机网络、物理学中的粒子结构、社交网络以及食物链都可以用图结构来表达。这些域本质上都是网络或者图，也意味着，在这些域中，我们可以得到不同的事件或者实体之间的关系。图是一种解决关系问题时的通用语言，各种情况下的统一数学表示。将问题抽象成图，可以用同一种机器学习算法解决所有问题。在复杂领域中存在的丰富的关系结构，可以被表示为关系图relational机器学习是通过数据及先验知识，自动改进计算机算法的研究。人类的学习过程是，通过归纳总结以往的经验，形成规律。在遇到新的问题时，通过规律，预测结果或未来。机器学习与人类的学习过程类似，机器学习的重点是数据与模型。机器学习研究的是从数据中通过选取合适的算法，自动的归纳逻辑或规则，并根据这个归纳的结果模型，输入新的数据来预测未来。真实世界的数据是有噪声的，有许多复杂的子结构。像“在这幅图中勾勒出人物轮廓”这样的任务对人类来说很容易，但很难转化为单个算法。深度学习允许我们将大量的数据转换为某种函数，从而实现特定任务的自动化。计算机可以处理人类无法处理的数据量(由于所需的时间或精力)。这使得新的分析成为可能，例如分析数十亿个交易网站的指纹欺诈。图机器学习就是将机器学习应用于图数据。图数据往往会包含大量有价值的关系数据。然而，许多之前的机器学习模型往往只关注每个样本的特征，而没有考虑到样本之间的关系数据或没有很好的方法来利用和建模这些关系数据。图机器学习为我们提供了利用这些关系数据的方法，它使得我们可以同时考虑图中每个节点的自身特征、邻节点以及邻节点的特征，以获取更好的性能。在传统机器学习流程中，我们需要对原始数据进行特征工程，例如手动提取特征等。但是现在我们可以直接使用图表示学习的方式来自动学习到数据的特征，直接应用于下流的预测任务。图的表示学习可以学习到图数据用于机器学习的，且与下游任务无关的特征，我们希望这个向量能够抓住数据的结构信息。这个数据被称作特征表示(featureNetworks，GNN)是一类基于深度学习的处理图域信息的方法，深度学习领域关于图神经网络的研究热情日益高涨，图神经网络已经成为各大深度学习顶会的研究热点。图神经网络(GNN)的概念最早是Scarselli等人在2009年提出的，它扩展了现有的神经网络，用于处理图中表示的数据。在图中，每个节点是由其特性和相关节点定义的。对于图结构而言，并没有天然的顺序而言，如果使用顺序来完整地表达图的话，那么就需要将图分解成所有可能的序列，然后对序列进行建模。显然，这种方式非常的冗余以及计算量非常大。与此相反，GNN采用在每个节点上分别传播(propagate)的方式进行学习，由此忽略了节点的顺序，相当于GNN的输出会随着输入的不同而不同。图结构的边表示节点之间的依存关系。然而，传统的神经网络中，依存关系是通过节点特征表达出来的。传统的神经网络不能显式地表达这种依存关系，而是通过不同节点特征来间接地表达节点之间的关系。通常来说，GNN通过邻接节点的加权求和来更新节点的隐藏状态。AlphaFold系统是DeepMind于2020年在CASP14大赛上推出的GNN网络，它在蛋白质结构预测方面表现出了无与伦比的正确性。AlphaFold将被折叠的蛋白质视作是一个空间图(spatialgraph)，氨基酸残基视作节点，在相近的节点之间建立边，形成图结构，搭建深度学习模型，预测节点在空间中的位置。在接收到这个单一序列的输入之后，AlphaFold通过深度神经网络，预测这一个氨基酸链条会如何折叠，输出一个拓扑结构。这是一个图神经网络的应用。作者将建筑空间的关系表现成图结构数据，用节点表示空间位置，连边表示空间的相连关系，然后训练图卷积神经网络（DGCNN）来预测建筑的标签属性。比较可惜的是，作者使用的数据集是脚本生成的，而不是真实的建筑数据，建筑类型也仅限于规整方形的建筑。GNN，可以提高针对矢量图形的识别率。Dual-Stream可以在每一层神经元上，聚合到边缘方向的特征，且不会因提前聚合特征而影响到性能。实验表明，基于YOLaT的矢量图形识别效果，要优于其它的GNN或CNN方法。品览是AI建筑设计智造者，专注于建筑设计AI服务，致力于为地产企业和设计院客户提供AI设计出图服务。自主研发的建筑AI智能设计云平台AlphaDraw「筑绘通」，基于计算机视觉技术，建筑设计知识库和生成式强化学习算法帮助客户自动完成施工图设计。仅需上传建筑方案图纸就可以自动完善成套施工图，并符合各地设计规范，助力企业标准化出图、效率质量双提升。

标题：对抗性机器学习的初学者指南
https://baijiahao.baidu.com/s?id=1722543457444517112&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
learning)主要是指在对攻击者的能力、及攻击后果的研究与理解的基础上，设计各种能够抵抗安全挑战(攻击)的机器学习(ML)算法。prediction的访问权限，就会窃取那些远程部署的机器学习模型的副本。此类攻击的背后原理是，攻击者通过输入，向目标模型发出请求，以提取尽可能多的信息，并使用掌握到的输入和输出集，来产生并训练一个替代模型(substitute当然，该提取模型在实施的过程中会有一定的困难，攻击者需要通过强劲的计算能力，来重新训练具有准确性和保真度的新模型，以从头开始替代现有的模型。在最终用户与PRADA(https://arxiv.org/abs/1805.02628)等模型之间构建代理。推理攻击旨在通过反转机器学习模型中的信息流，以方便攻击者洞察到那些并未显示共享的模型。从统计学的角度来说，由于私有的机密数据往往与公开发布数据有着潜在的相关性，而机器学习的各种分类器(classifier)具有捕捉到此类统计相关性的能力，因此推理攻击会给个人和系统构成严重的隐私和安全威胁。Dropout(译者注：在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率，将其暂时从网络中丢弃)等技术。投毒攻击技术是指攻击者在训练数据集中插入损坏性数据，以在训练的过程中破坏目标的机器学习模型。计算机视觉系统在面对特定的像素模式进行推理时，会被此类数据投毒技术触发某种特定的行为。当然，也有一些数据投毒技术，旨在降低机器学习模型在一个或多个输出类别上的准确性。由于此类攻击可以在使用相同数据的不同模型之间进行传播，因此它们在训练数据上执行的时候，很难被检测到。据此，攻击者会试图通过修改决策边界，来破坏模型的可用性，从而产生各种不正确的预测。此外，攻击者还会在目标模型中创建后门，以便他们创建某些特定的输入，进而产生可被操控的预测，以及方便后续攻击的结果。攻击者通过插入一个细微的扰动(如某种形式的噪声)，并转换成某个机器学习模型的输入，并使之产生分类错误。虽然与投毒攻击有几分类似，但是规避攻击主要尝试的是，在推理阶段、而不是在训练中，利用模型的弱点。当然，攻击者对于目标系统的了解程度是至关重要的。他们对于目标模型、及其构建方式越了解，就越容易对其发起攻击。example)”时。这往往是一种被“精心构造”扰动性输入。它看似与那些未被篡改的副本相同，但实际上完全避开了正确的分类器。Toolbox，ART,https://github.com/Trusted-AI/adversarial-robustness-toolbox)是一种可用于机器学习的安全类Python库。由ART提供的工具可让开发与研究人员，去评估和防御应用程序、及其所用到的机器学习模型，进而抵御上面提到的四类威胁与攻击。ART能够支持时下流行的机器学习框架，例如：TensorFlow、Keras、PyTorch、以及scikit-learn。而它支持的数据类型包括：图像、数据表、音频、以及视频等。同时，它还支持分类、物体检测、以及语音识别等机器学习任务。您可以通过命令：pipAdversarialTrainermodel.compile(loss=keras.losses.categorical_crossentropy,是一种命令行工具和通用的自动化层级，可以被用于评估机器学习系统的安全性。基于ART和TextAttack(译者注：是一种Python框架，可用于NLP中的对抗性攻击、数据增强和模型训练)的Counterfit，是专为机器学习模型的安全审计而开发的，可实现黑盒式的规避算法。Counterfit包括如下实用命令：--------------------------------------------------------Microsoft#ATML--------------------------------------------------------list

标题：机器学习简介
http://baijiahao.baidu.com/s?id=1718680870406595913&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
你是否曾经听到过人们谈论机器学习，而你却对其含义只有一个模糊的概念呢？你是否已经厌倦了在和同事对话时只能点头呢？现在，让我们一起来改变这个现状吧！这篇指南是为那些对机器学习感兴趣，但又不知从哪里开始的人而写的。我猜有很多人曾经尝试着阅读机器学习的维基百科词条，但是读着读着倍感挫折，然后直接放弃，希望能有人给出一个更直观的解释。本文就是你们想要的东西。本文的写作目标是让任何人都能看懂，这意味着文中有大量的概括。但是那又如何呢？只要能让读者对机器学习更感兴趣，这篇文章的任务也就完成了。Algorithms）[1]就能告诉你一些关于你数据的有趣结论。不用编码，你将数据输入泛型算法当中，它就会在数据的基础上建立出它自己的逻辑。比如说，有一种算法被称为分类算法，它可以将数据分为不同的组。分类算法可以用来识别手写数字；不用修改一行代码，它也可以用来区分垃圾邮件和非垃圾邮件。如果给同样的算法输入不同的训练数据，它就能得出不同的分类逻辑。假设你是一名房地产经纪人，你的生意蒸蒸日上，因此你雇了一批新员工来帮忙。但是问题来了——虽然你可以一眼估算出房子的价格，但新员工却不像你这样经验丰富，他们不知道如何给房子估价。为了帮助你的新员工（也许就是为了给自己放个假嘻嘻），你决定写一个可以根据房屋大小、地段以及同类房屋成交价等因素来评估一间房屋的价格的小软件。近三个月来，每当你的城市里有人卖了房子，你都记录了下面的细节——卧室数量、房屋大小、地段等等。但最重要的是，你写下了最终的成交价：为了编写你的软件，你将包含每一套房产的训练数据输入到你的机器学习算法当中去。算法会尝试找出需要做哪些数学运算来得出价格。你能从这张图里看出来测验中的数学题是怎样的吗？你知道自己应该对左边的数字「做些什么」，才能得到右边的答案。中，你让计算机为你算出这种关系。而一旦你知道了解决这类特定问题所需要的数学方法后，你就可以解答其它同类问题了！让我们回到房地产经纪人的例子。如果你不知道每栋房子的售价怎么办？即使你所知道的仅仅是每栋房屋的大小、位置等信息，你也可以搞出一些很酷炫的花样来。这就是我们所说的非这就有点像有人给你一张纸，上面写了一列数字，然后说：「我不太清楚这些数字有什么意义，但也许你能找出些规律或是把它们分类什么的——祝你好运！」所以该怎么处理这些数据呢？首先，你可以用个算法自动从数据中划分出不同的细分市场。也许你会发现，当地大学附近的购房者特喜欢户型小、卧室多的房子，而郊区的购房者偏好三卧室的大户型。了解这些不同消费者的喜好可以直接帮助你的营销。你还可以做件很酷炫的事，就是自动找出非同寻常的房屋。这些与众不同的房产也许是奢华的豪宅，而你可以将最优秀的销售人员集中在这些地区，因为他们的佣金更高。在接下来的内容中我们主要讨论监督式学习，但这并不是因为非监督式学习比较没用或是无趣。实际上，随着算法的改良，非监督式学习正变得越来越重要，因为即使不将数据和正确答案联系在一起，它也可以被使用。作为人类的一员，你的大脑可以应付绝大多数情况，并且在没有任何明确指令时也能够学习如何处理这些情况。如果你做房地产经纪人时间足够长，你对于房产的合适定价、房屋的最佳营销方式以及客户会感兴趣类型等等都会有一种本能般的「感觉」。但是目前的机器学习算法还没有那么强大——它们只能在非常特定的、有限的问题上有效。也许在这种情况下，「学习」更贴切的定义是但是「机器在少量样本数据的基础上找出一个公式来解决特定的问题」不是个好名字。所以最后我们用「机器学习」取而代之。年后的未来读的这篇文章，而我们人类也已经得出了强人工智能的算法的话，那这篇文章看起来就像个老古董了。那样的话，就别读了，去让你的机器佣人给你做份三明治吧，未来的人类。假如你像这样瞎忙几个小时，最后也许会得到一些像模像样的东西。但是你的程序永不会完美，而且当价格变化时很难维护。如果能让计算机找出实现上述函数功能的办法，岂不更好？只要返回的房价数字正确，谁会在乎函数具体干了些什么呢？考虑这个问题的一种角度是将价格看作一碗美味的汤，而汤的原材料就是卧室数量、面积和地段。如果你能算出每种原材料对最终的价格有多大影响，也许就能得到各种原材料混合形成最终价格的具体比例。0，你的函数就完美了。它意味着，根据输入的数据，你的程序对每一笔房产交易的估价都是分毫不差。所以这就是我们的目标——通过尝试不同的权重值，使代价尽可能的低。挺简单的，对吧？想一想刚才你做了些什么。你拿到了一些数据，将它们输入至三个泛型的、简单的步骤中，最后你得到了一个可以对你所在区域任何房屋进行估价的函数。房价网站们，你们要小心了！年来，很多领域（如语言学、翻译学）的研究表明，这种「搅拌数字汤」（我编的词）的泛型学习算法已经超过了那些真人尝试明确规则的方法。机器学习的「笨」办法终于打败了人类专家。你最后写出的程序是很笨的，它甚至不知道什么是「面积」和「卧室数量」。它知道的只是搅拌，改变数字来得到正确的答案。你可能会对「为何一组特殊的权重值会有效」一无所知。你只是写出了一个你实际上并不理解却能证明有效的函数。试想，如果你的预测函数输入的参数不是「面积」和「卧室数量」，而是一列数字，每个数字代表了你车顶安装的摄像头捕捉的画面中的一个像素。然后，假设预测的输出不是「价格」而是「方向盘转动角度」，这样你就得到了一个程序可以自动操纵你的汽车了！好吧，当然你不可能试遍所有权重组合来找到效果最好的组合。直到世界毁灭你也算不完，因为这数字和组合无穷无尽。图中，蓝色的最低点就是代价最低的地方——在这里我们的程序偏离最小。最高点们意味着偏离最大。所以，如果我们能找到一组权重值让我们到达图中的最低点，我们就得到了答案！因此，我们需要做的只是调整我们的权重，使得我们在图上朝着最低点「走下坡路」。如果我们不断微调权重，一直向最低点移动，那么我们最终不用尝试太多权重就可以到达那里。如果你还记得一点微积分的话，你也许记得如果你对一个函数求导，它会告诉你函数任意一点切线的斜率。换句话说，对于图上任意给定的一点，求导能告诉我们哪条是下坡路。我们可以利用这个知识不断走向最低点。[5]所以，如果我们对代价函数关于每一个权重求偏导，那么我们就可以从每一个权重中减去该值。这样可以让我们更加接近山底。一直这样做，最终我们将到达底部，得到权重的最优值。（读不懂？不用担心，继续往下读）。。你在估算一个能够拟合所有房价数据点的直线表达式。然后，你再根据房子可能在你的直线上出现的位置，利用这个等式来估算你从未见过的房屋的价格。这是一个十分强大的想法，你可以用它来解决「实际」问题。但是，尽管我展示给你的这种方法可能在简单的情况下有效，它却不能应用于所有情况。原因之一，就是因为房价不会是简简单单一条连续的直线。不过幸运的是，有很多办法来处理这种情况。有许多机器学习算法可以处理非线性数据（如神经网络或带核函数的支持向量机）。除此之外，灵活使用线性回归也能拟合更复杂的线条。在所有的情况下，寻找最优权重这一基本思路依然适用。另外，我忽略了过拟合（overfitting）的概念。得到一组能完美预测原始数据集中房价的权重组很简单，但用这组权重组来预测原始数据集之外的任何新房屋其实都不怎么准确。这也是有许多解决办法的（如正则化以及使用交叉验证的数据集）。学习如何应对这一问题，是学习如何成功应用机器学习技术的重点之一。换言之，尽管基本概念非常简单，要通过机器学习得到有用的结果还是需要一些技巧和经验的。但是，这是每个开发者都能学会的技巧。一旦你开始明白，用机器学习技术解决那些看似困难问题（如字迹识别）有多便利时，你就会有一种，只要有足够的数据，你就能够用机器学习解决任何问题的感觉。只需要输入数据，计算机就能神奇地找出拟合数据的等式！例如，如果你建立了一个根据每套房屋内盆栽种类的数量来预测房价的模型，那它永远都不会有效果。因为盆栽种类的数量和房价之间没有任何的关系。所以，无论你多卖力地尝试，计算机永远也推导不出两者之间的关系。所以请记住，如果一个问题人类专家不能手动用数据解决，计算机可能也不能解决。然而，对于那些人类能够解决的问题，如果计算机能够更快地解决，那岂不美哉？我认为，目前机器学习的最大问题是它主要活跃于学术界和商业研究组织中。对于只想大体了解一下，而不打算成为专家的人们来说，简单易懂的资料不多。但是这种情况每天正在改善。上的免费机器学习课程非常棒。我强烈建议从此入手。对于任何拥有计算机或科学学位的人，或是还能记住一点点数学的人来说，都应该非常容易入门。boosting是一种集成技术，试图从多个弱分类器中创建强分类器。通过从训练数据构建一个模型，然后创建第二个模型试图纠正第一个模型中的错误。不断添加模型，直到训练集被完美地预测或者添加到最大数量。AdaBoost是第一个为二分类开发的真正成功的提升算法。现代boosting方法建立在AdaBoost上，最著名的是随机梯度提升机（stochasticAdaBoost用于短决策树。创建第一棵树之后，使用树在每个训练实例上的性能来得到一个权重，决定下一棵树对每个训练实例的注意力。难以预测的训练数据被赋予更多权重，而易于预测的实例被赋予更少的权重。模型是一个接一个地顺序创建的，每个模型更新训练实例上的权重，这些权重影响序列中下一个树所执行的学习。构建完所有树之后，将对新数据进行预测。Adaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器(强分类器)。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。使用adaboost分类器可以排除一些不必要的训练数据特征，并放在关键的训练数据上面。对adaBoost算法的研究以及应用大多集中于分类问题，同时也出现了一些在回归问题上的应用。就其应用adaBoost系列主要解决了:的方法之一，bosting就是把若干个分类效果并不好的分类器综合起来考虑，会得到一个效果比较好的分类器。下图，左右两个决策树，单个看是效果不怎么好的，但是把同样的数据投入进去，把两个结果加起来考虑，就会增加可信度。

标题：小白入门机器学习的三个问题|文末福利
http://baijiahao.baidu.com/s?id=1717957939000638996&wfr=spider&for=pc
keywords  0 grammer  2 
get_summary
按照字面意思理解，“机器学习”就是让机器自己学会某种东西。更准确一点就是：让计算机程序（机器）不是通过人类直接指定的规则，而是通过自身运行，习得（学习）事物的规律和事物间的关联。对于人类而言，一个概念对应的是具体的事物，我们认知的事物都不是孤立的，互相之间有着各种各样的关联。比如我们对一个人说“苹果”的时候，他可能会想乔布斯创立的公司；也有可能想到圆圆的、香甜的、有皮有核的水果；也有可能想到香蕉、菠萝等其他水果。但是如果我继续说：“苹果一定要生吃，蒸熟了再吃就不脆了。”那么“苹果”一词确定无疑指的是水果，而不是公司。因为在我们的知识库里，都知道水果可以吃，但是公司不能吃。出现在同一句话中的“吃”对“苹果”起到了限定作用，这是人类的理解。如果我们将“苹果”这两个字输入计算机，计算机并不会幻视出一个水果，也不会像人那样“意识到”这个单词的含义。计算机程序能够处理的只有数值和运算。数字通过逻辑电路进行若干运算后，生成计算结果。所以要让一段程序了解客观世界变化万千的事物，则必须将这些事物转化为数值，将事物的变化和不同事物之间的关联转化为运算。当若干现实世界的事物转换为数值后，计算机通过在这些数值之上的一系列运算来确定它们之间的关系，再根据一个全集之中个体之间的相互关系来确定某个个体在整体（全集）中的位置。产生关联的还有若干数值，它们对应的概念可能是“香蕉”（Vb）、“菠萝”（Vp）、“猕猴桃”（Vc）等。据此，计算机就会发现int、double、float……但实际上，如果将一个语言要素对应成一个标量的话，太容易出现两个原本相差甚远的概念经过简单运算后相等的情况了。假设“苹果”被转化为这种现实世界和计算机之间从概念到数值，从关系到运算的映射，造就了机器可以通过自主学习获得事物规律的可能。既然机器有可能自己学习事物的规律，那么如何才能让它学到规律呢？我们先来看一个故事。在这个故事里，小猫就是一个基于规则的（rule-based）“计算机程序”，它完全按照“开发者”猫妈妈的指令行事。但是因为次都得出了错误的结果。如果要把小猫变成一个基于机器学习模型的（model-based）计算机程序，猫妈妈该怎么做呢？猫妈妈可以先告诉小猫：要注意老鼠的耳朵、鼻子和尾巴。小猫通过对比发现：老鼠的耳朵是圆的，别的动物耳朵不是圆形的；老鼠都有长而细的尾巴，别的动物有的尾巴短，有的尾巴粗；老鼠的鼻子是尖的，别的动物不一定是这样的。这时小猫就学习到一个规律——老鼠是圆耳朵、细长尾巴、尖鼻子的动物，通过这个规律来抓老鼠，那么小猫就成了一个“老鼠分类器”。小猫（此处将其类比为一个计算机程序）是机器（machine），让它成为“老鼠分类器”的过程叫作学习（learning）。猫妈妈给小猫看的那些照片是用于学习的数据（data）。猫妈妈告知小猫要注意的几点，是这个分类器的特征（feature）。学习的结果“老鼠分类器”是一个模型（model）。小猫思考的过程就是算法（algorithm）。很多同学想学机器学习，但是一上来就看模型，看到一大堆炫酷的公式，难免感觉很吓人。有些人因此萌生退意，要么放弃，要么只用现成工具把模型当作黑盒使用。其实，学习经典模型，并不需要多么深厚的数学功底，只要掌握本科阶段所教授的数学知识就基本够用了。在学习的最初阶段，只要满足以下几个条件，就可以对经典机器学习模型有一定深度的感性认识了。如果想要灵活运用机器学习，还需要进一步学习数学知识。建议大家在学习模型的过程中，一旦遇到了数学上的阻碍，就去查找相应知识，制作一本自己的数学知识速查手册，把常用的细小知识点都记录下来，按主题整理成速查手册（小字典），需要用的时候快速查找对应的知识点，这样我们学习机器学习的过程会顺畅不少。等，这些选定的模型，一定要搞清楚其问题域、模型函数、目标函数、训练算法……深入数学公式推导的层面，理解每一步的公式变换和对应的物理意义，然后去实践。达到掌握机器学习精髓的程度并非一蹴而就，总要从最简单的模型开始。即使是掌握最简单的模型，也需要反复学习。有可能第一遍看完有点似懂非懂，或者感觉自己明白了，但要从头推导又卡壳了。就像刚学完又忘掉的生字，或是背了一半，后半部分怎么也想不起来的九九乘法口诀。这都是非常正常的现象。究其原因，就是还没有真正掌握。多学几遍，从头到尾掌握整件事情的逻辑，知道一个模型是怎么从最初设定目标，一步步实现目标的。当真正掌握了这个过程之后，再回头看一个个具体的模型，每一步都是顺理成章的。虽然我们学习的是机器学习原理，但是这并不等于我们就可以停留在原理层面。为了学好原理，我们需要具备基本的编程能力，至于编程语言的选择，如果你在开始学习的时候已经有了编程基础，那么继续使用自己擅长的语言就可以。如果你还不会编程，或者觉得自己之前学的、用的语言不好，想换一种更适合机器学习的编程语言，那么我的建议是

标题：机器学习浅谈(一)
https://baijiahao.baidu.com/s?id=1717756321003304129&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
各位小伙伴大家好！今天带领大家开一个新坑，我们一起扒一扒机器学习这个热点概念。希望能帮助大家理解机器学习，在自己的研究工作中使用机器学习。就算不能直接帮大家冲击更高水平的期刊，至少也能理解那些使用机器学习的人在干什么，再不济可以跟不懂机器学习的小白吹吹牛。但是笔者也是半路出家的小白，虽说在相关领域也有了几年工作经验，但是难免有遗漏和失误之处，希望大家多批评指正。废话不多说，我们直奔主题。Samuel提出的。但是受限于当时的计算机算力，他的神经网络模型只有一层，所以能从事的工作有限，因此这个概念没有得到广泛的推广和认可。在1969年Minsky更是证明了这个复杂的机器学习模型只能解决的线性问题，这无疑等同于给这个方法判了死刑。此后仍未放弃的研究者将这个方法不断进行改善，引入了非线性激活函数，也将神经网络的结构拓展到了三层，极大的拓宽了机器学习的应用空间。并终于在1997年由Mitchell写出了机器学习的经典教科书，这个研究方向才算回归到了大众视野中。而近十年机器学习的也乘着计算机科学爆炸发展的东风一举突破了三层神经网络的限制，真正意义上实现了具备自主的“思考”能力的算法，并被广泛的应用到了我们的生活中，比如翻译软件，p图软件等等。然而，机器学习作为一个新兴学科，其定义尚未达成全员的共识，但其中一个广为接受的定义是由Mitchell提出的：机器学习是一种可以自发的利用经验（E）提高针对特定任务（T）的表现（P）的算法。用一个比较容易理解的方式来说，机器学习并不神秘，它就是一种算法。这种算法是被用于执行特定任务而设计出来的，既然是为了执行特定任务，那目标自然是提升它在这个任务中的表现。直到这里，它和普通的算法没任何区别。但接下来的特征却让机器学习和一般算法产生了本质区别。即由机器学习的定义出发，我们可以轻易区分两组特别容易混淆的概念。首先是机器学习和人工智能：人工智能是一个庞大的概念，而机器学习只是人工智能的算法部分。另一个是机器学习和深度学习：机器学习泛指所有满足定义的算法，但一般认为超过三层的神经网络才可以被称为深度学习。而发现自己做的不是深度学习的小伙伴也大可不必担忧，因为深度学习也不一定就比一般的机器学习表现更好。简单总结一下，今天我们了解了机器学习的历史和定义。接下来我们会继续探讨机器学习的分类，请大家拭目以待。

标题：机器学习AI数据集产品汇总(二)
https://baijiahao.baidu.com/s?id=1735781041872736777&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
随着人工智能的不断发展，数据标注作为人工智能发展道路上的基石，是人工智能发展中的重要一环。数据标注的过程就是通过人工贴标签的方式，为机器提供可学习的样本数据，使机器可以自主识别数据。定制化采集标注数据需要花费大量的人力、物力，购买现有的数据集成为了许多企业的优先选择。景联文科技作为专业的数据采集标注公司，先后建立杭州数据总部，武汉、金华、衡阳等不同省市数据处理分部，采取阿米巴内部竞争管理模式，构建全国229840张图像，2210人，每个人4张情绪图像，包括愤怒、惊讶、恐惧、厌恶，每人50张室内姿态生活照，50张室外姿态生活照景联文科技还可以针对特定人群、特定场景、特定数据类型提供个性化的数据定制服务，我们将全力协助客户得到满意的数据服务。2012年，有科研背景以技术发展为导向的高新技术企业和AI基础数据服务企业。为全球数千家人工智能从业公司和高校科研机构提供AI数据采集、数据标注、数据集产品、标注平台定制开发、假指纹采集和指纹防伪算法服务。景联文始终践行“做全球AI行业客户的数据参谋”的企业使命，助力人工智能技术加速数字经济相关产业质量变革、动力变革与效率变革，赋能传统产业智能化转型升级。

标题：什么是机器学习
https://baijiahao.baidu.com/s?id=1726996937092464099&wfr=spider&for=pc
keywords  0 grammer  0 
get_summary
机器学习是人工智能的一个子集。这项技术的主要任务是指导计算机从数据中学习，然后利用经验来改善自身的性能，不需要进行明确的编程。在机器学习中，算法会不断进行训练，从大型数据集中发现模式和相关性，然后根据数据分析结果做出最佳决策和预测。机器学习应用具有自我演进能力，它们获得的数据越多，准确性会越高。机器学习技术的应用无处不在，比如，我们的家居生活、购物车、娱乐媒体以及医疗保健等。机器学习及其分支深度学习和神经网络都属于人工智能的子集。人工智能是基于数据处理来做出决策和预测。借助机器学习算法，人工智能不仅能够处理数据，还能在不需要任何额外编程的情况下，利用这些数据进行学习，变得更智能。是父集，包含了机器学习的所有子集。人工智能下面的第一个子集是机器学习，深度学习是机器学习的一个分支，神经网络则是深度学习的基础结构。包含多种使用不同算法的学习模型。根据数据的性质和期望的结果，可以将学习模型分成四种，分别是监督学习、无监督学习、半监督学习和强化学习。而根据使用的数据集和预期结果，每一种模型可以应用一种或多种算法。机器学习算法主要用于对事物进行分类、发现模式、预测结果，以及制定明智的决策。算法一般一次只使用一种，但如果处理的数据非常复杂、难以预测，也可以组合使用多种算法，以尽可能提高准确度。机器学习算法能够识别模式和相关性，这意味着它们可以快速准确地分析自身的投资回报率。对于投资机器学习技术的企业来说，他们可以利用这个特性，快速评估采用机器学习技术对运营的影响。下面列举了一小部分快速发展的企业机器学习应用领域。70%。推荐引擎已经广泛应用于各种零售和购物平台。在流媒体音乐和视频服务领域，推荐引擎肯定也会有自己的一席之地。要发掘销售线索并引导其通过销售漏斗的各个阶段，企业需要采集和分析尽可能多的客户数据。从聊天记录到上传的图片，现代消费者产生了大量不同的非结构化数据。借助机器学习应用，营销人员可以更好地理解这些数据，并利用这些数据提供个性化的营销内容，与现有客户和潜在客户开展实时互动。包含许多不同的数据集，比如销售业绩统计信息、消费者评论、市场趋势报告和供应链管理记录等。企业可以利用机器学习算法从这些数据中发现相关性和模式。而这些洞察几乎可以应用于每个业务领域，比如，优化网络内物联网设备的工作流、更高效地将重复性任务或易出错任务实现自动化。和智能工厂都在越来越多地利用物联网设备和机器，并且在所有运输队伍和运营团队之间使用云连接。故障和效率低下问题会导致巨大的成本损失和业务中断。如果手动采集维护和维修数据，那么企业几乎不可能预测潜在问题，更不用说自动预测和预防潜在问题。物联网网关传感器甚至可以安装到已有几十年历史的模拟机器上，提高整个企业的可视性和效率。

标题：“太极”助力,腾讯广告如何借大模型降本增效?
https://www.pcpop.com/article/6748187.shtml
keywords  0 grammer  3 
get_summary
更低成本、更优效果，也就是“降本增效”，是所有广告投放追求的目标。广告技术的发展正让“降本增效”越来越具体、越来越可以量化和感知，比如目前互联网广告平台开始以GMV（成交金额）或ROI（投入产出比）为营销效果的评估标准。一次更高效的广告投放，本质上是在合适的场景，让对的广告出现在对的人面前。这离不开广告平台对广告内容和用户群体的深刻理解，并在他们之间达成更准确的匹配。腾讯广告已经为此交出了一份答卷：首先，以国际领先的混元AI大模型助力系统深刻理解广告内容，其次以精排大模型提升广告和用户的匹配准确率。此外，这两者的底层都离不开一个业内领先的机器学习平台——太极。早期的深度学习模型大约有六千万参数，大约230MB大小，能够把日常物体分成一千个类别；今年火热的绘画AI“DALL-E2”则已经有大约70亿参数，大约26GB大小，输入一句描述句子就能画出一张对应的画，内容准确、视觉效果优美，体现出顶级的文本和图像理解水平。广告中既有艺术和创意，也有感情和期待，推荐系统要首先能够理解广告中蕴含的丰富信息，才能做出恰当的推荐。腾讯自研的深度学习大模型——腾讯广告混元AI大模型就是广告系统理解内容的核心引擎。腾讯广告混元AI大模型，是一个具有千亿参数的大模型，能够准确理解文字和图像中蕴含的各个层面的信息；它甚至可以把文字、图像、视频作为一个整体来理解，这样不仅对广告的理解更准确，也更符合平台用户对广告的整体感受。大模型和类似的图文理解模型做过全面的正面对比，目前横扫跨模态检索领域5大权威测评集大满贯、多模态理解领域国际权威榜单VCR上排名第一、CLUE自然语言理解分类榜及CLUE总榜登顶，比大部分模型的图文综合理解能力还要强，是国内当之无愧的多模态、跨模态AI大模型。混元AI大模型的多模态理解能力，可以有效的加深推荐系统对于广告的理解，从而更精准的将广告推荐给合适的人群，提高用户体验以及广告转化效果。除了理解已有的广告内容，混元AI大模型还有文字&图像&视频综合生成能力，已经以产品的形式在腾讯广告投放平台提供给每一位广告主，可以极大的提升广告制作的效率。在理解广告之后，广告平台还面临一个挑战：把广告展示给合适的人群。模型在解决这一挑战中发挥了核心作用，特别是精排模型。广告平台上有数以万计的广告主和数以亿计的用户，即便经过广告定向的筛选，匹配一次用户请求的广告仍然可能有成千上万，具体向用户展示哪一个或者哪几个广告才能在尽量符合用户偏好的同时为广告主带来最大的收入，这就是一个极为复杂的多对多匹配问题，需要精细的预估和排序。腾讯广告精排大模型就是为了解决这个难题而设计的。单模型推理参数达千亿级别，序列化后大小可达数百GB，浮点数计算量最高每秒超过10亿次，在行业位于先进水平。如此大的模型，最明显的收益是可以利用更多的特征和样本数据，学习得到更强大更精确的模型，进而实现更高效的匹配，比如：2、可以基于更多场景、页面、上下文信息，在跨场景联合建模的同时强化场景差异性表达，降低维护成本，提升用户广告体验；3、可以基于更长期的样本数据，配合恰当的模型结构和学习算法，平衡不同行业不同稀疏程度广告主投放目标的学习，提升投放效果；腾讯广告精排大模型各项算法指标相对于百亿规模小模型有显著提升，全流量上线后，用户可以看到更符合自己兴趣的广告，广告主也可以期待有更高的投放回报。相比以前的小模型算法，腾讯广告精排大模型已累计给广告主带来15%的GMV提升。精排大模型迄今为止的效果提升还只是一个开始，大模型平台系统和模型算法会持续升级，与此同时，大模型能力也会逐步向召回、粗排等其他环节辐射，不断抬高效果的天花板，为用户和广告主提供更佳的广告体验。在实现高效广告匹配的同时，腾讯广告也是隐私保护的技术探索者和实践者。早在2019年，腾讯广告便获得ISO/IEC29151两项国际认证，在信息安全和隐私保护能力与国际主流标准全面接轨。此外，腾讯广告也在联邦学习等隐私计算技术持续深耕，助力广告程序化交易联合建模，在保障双方的数据安全前提下实现合作。腾讯广告平台需要为亿级的用户、海量并不停增加的广告内容提供服务。大模型+高访问压力，对广告平台的承载能力和计算能力都提出了很高的要求。实际上腾讯广告精排大模型的要求尤其苛刻，不仅模型大小是业界顶级，还需要在用户等待页面加载的极短时间内就完成广告匹配。为了满足这些规模和性能要求，腾讯专门自研搭建了业界一流的太极机器学习平台，可支持10TB级模型训练、TB级模型推理和分钟级模型发布上线，扩展集群规模则可支持更大的模型训练和推理，为实际业务提供大模型的情况下，同时具有很高的性能，达到行业领先水平。太极机器学习平台采用了分布式参数服务器架构，这是业界第一梯队企业们公认的最佳选择。这种架构的特点是，存储模型参数和执行模型计算，这两种任务在分别的服务器上运行，增加更多服务器就可以支持更大、计算需求更高的模型。太极机器学习平台中的参数服务器系统AngelPS也是腾讯自研的成果，现在不仅可以承载10TB级模型的训练，对多维特征融合、复杂模型结构等更高级、更前瞻性的功能也有优秀支持。同时太极机器学习平台还具备超大模型在线推理服务的能力。推理计算方面，不仅支持常规的CPU计算，还支持复杂模型的GPU计算加速；存储上，根据在线推理模型大小，自适应选择小模型本地内存加载和大模型AngelPS远程查询的部署策略；在线服务运营上，平台具备多级容灾能力，脏模型秒级回滚能力，以及完备的系统和业务运行实时指标监控和报警系统。规模预测的腾讯广告精排大模型能短时间内在太极上完成计算、提供可靠的广告匹配结果。不仅如此，为了提供周全的可靠性、紧跟潮流的匹配能力，上百GB规模的腾讯广告精排大模型需要有多个副本同时运行，每天还需要根据当天的真实用户行为持续学习，这些都依赖太极机器平台强大的承载能力和周全的功能。太极机器学习平台有突出的模型训练硬件加速能力。以混元AI大模型的训练为例，太极的加速方案高于业界其他方案可以把太极机器学习平台比作一条多车道的高速公路，大模型就像许许多多的重型卡车，可以在高速公路上高速行驶；相比之下，较落后的技术平台就像是乡间的泥泞小道，只通行一辆小车也颠簸、缓慢。两者能提供的服务以及最终带来的用户体验显然也不能同日而语，而当前具备领先地位的太极机器学习平台，毫无疑问能够帮助提供更好的基建体系，助力大模型们高速运行。在腾讯太极机器学习平台之上，混元AI大模型和腾讯广告精排大模型共同完善优化了广告理解、用户理解、广告和用户匹配的整个流程，提升对广告的理解，提升广告和人群的匹配效率，提升广告主的推荐精准度和转化效率。在“降本增效”的目标指引下，关注并提升GMV的腾讯广告平台技术已经打赢了大模型关键战役的第一仗。未来腾讯广告也将不断基于此优化模型，帮助广告主达成更高GMV。特别提醒：本网信息来自于互联网，目的在于传递更多信息，并不代表本网赞同其观点。其原创性以及文中陈述文字和内容未经本站证实，对本文以及其中全部或者部分内容、文字的真实性、完整性、及时性本站不作任何保证或承诺，并请自行核实相关内容。本站不承担此类作品侵权行为的直接责任及连带责任。如若本网有任何内容侵犯您的权益，请及时联系我们，本站将会在24小时内处理完毕。:010-59548436,010-59544810,17352615267,niuxiaotong@pcpop.com

标题：腾讯混元 AI 大模型落地广告投放,显著降本增效背后,三大技术引擎...
http://www.myzaker.com/article/62b150858e9f09537e030ced
keywords  0 grammer  3 
get_summary
一次更高效的广告投放，本质上是在合适的场景，让对的广告出现在对的人面前。这离不开广告平台对广告内容和用户群体的深刻理解，并在他们之间达成更准确的匹配。大模型的这些核心能力，对广告推荐同样大有裨益。广告中既有艺术和创意，也有感情和期待，推荐系统要首先需理解广告中蕴含的丰富信息，才能做出恰当的推荐。大模型，是一个具有千亿参数的大模型，能够准确理解文字和图像中蕴含的各个层面的信息。它可以把文字、图像、视频作为一个整体来理解，这样不仅对广告的理解更准确，也更符合平台用户对广告的整体感受。大模型的多模态理解能力，可以有效的加深推荐系统对于广告的理解，从而更精准的将广告推荐给合适的人群，提高用户体验以及广告转化效果。此外，该模型还有文字视频综合生成能力，已经以产品的形式在腾讯广告投放平台提供给每一位广告主，可以极大的提升广告制作的效率。在理解广告之后，广告平台还面临一个挑战：把广告展示给合适的人群。模型在解决这一挑战中发挥了核心作用，特别是精排模型。广告平台上有数以万计的广告主、数以亿计的用户，即便经过广告定向的筛选，匹配一次用户请求的广告仍然可能有成千上万。具体向用户展示哪一个或者哪几个广告才能在尽量符合用户偏好的同时为广告主带来最大的收入，如此大的模型，最明显的收益是可以利用更多的特征和样本数据，学习得到更强大更精确的模型，进而实现更高效的匹配，比如：可以基于更多场景、页面、上下文信息，在跨场景联合建模的同时强化场景差异性表达，降低维护成本，提升用户广告体验；可以基于更长期的样本数据，配合恰当的模型结构和学习算法，平衡不同行业不同稀疏程度广告主投放目标的学习，提升投放效果；腾讯广告精排大模型各项算法指标相对于百亿规模小模型有显著提升，全流量上线后，用户可以看到更符合自己兴趣的广告，广告主也可以期待有更高的投放回报。当前精排大模型的效果提升还只是一个开始，大模型平台系统和模型算法会持续升级，与此同时，大模型能力也会逐步向召回、粗排等其他环节辐射，不断抬高效果的天花板，为用户和广告主提供更佳的广告体验。两项国际认证，在信息安全和隐私保护能力与国际主流标准全面接轨。此外，腾讯广告也在联邦学习等隐私计算技术持续深耕，助力广告程序化交易联合建模，在保障双方的数据安全前提下实现合作。高访问压力，对广告平台的承载能力和计算能力都提出了很高的要求。腾讯广告精排大模型的要求尤其苛刻，不仅模型大小是业界顶级，还需在用户等待页面加载的极短时间内就完成广告匹配。级模型推理和分钟级模型发布上线，扩展集群规模则可支持更大的模型训练和推理，为实际业务提供大模型的情况下，同时具有很高的性能，达到行业领先水平。这种架构的特点是，存储模型参数和执行模型计算，这两种任务在分别的服务器上运行，增加更多服务器就能支持更大、计算需求更高的模型。远程查询的部署策略；在线服务运营上，平台具备多级容灾能力，脏模型秒级回滚能力，以及完备的系统和业务运行实时指标监控和报警系统。规模的腾讯广告精排大模型需要有多个副本同时运行，每天还需根据当天的真实用户行为持续学习，这些都依赖太极机器平台强大的承载能力和周全的功能。如果将太极机器学习平台比作一条多车道的高速公路，大模型就像许许多多的重型卡车，可以在高速公路上高速行驶；相比之下，较落后的技术平台就像是乡间的泥泞小道，只通行一辆小车也颠簸、缓慢。大模型和腾讯广告精排大模型共同完善优化了广告理解、用户理解、广告和用户匹配的整个流程，提升了对广告的理解、广告和人群的匹配效率，以及广告主的推荐精准度和转化效率。的腾讯广告平台技术已打赢大模型落地的关一仗。接下来，腾讯广告计划不断基于此优化模型，帮助广告主达成更高

标题：AI软件市场规模超50亿美元,掘金机会有哪些?
https://www.esmchina.com/news/9076.html
keywords  0 grammer  0 
get_summary
咨询机构IDC近日发布《2021年中国人工智能软件及应用市场研究报告》。数据显示，2021年全年中国人工智能软件及应用市场规模达52.8亿美元（约330.3亿元人民币）……咨询机构IDC近日发布《2021年中国人工智能软件及应用市场研究报告》。数据显示，2021年全年中国人工智能软件及应用市场规模达52.8亿美元（约330.3亿元人民币），相比2020年涨幅为43.1%。相比2020年同期预测值，涨幅略为降低，疫情的影响明显，以及行业端对AI投资日趋理性谨慎。无论是中国市场还是美国市场，近几年来，主流厂商的市场格局一直在不断演变，远没有达到格局稳定、几家公司形成垄断的阶段。在未来，技术创新型企业仍有可能胜出。IDC中国助理研究总监卢言霞表示：“人工智能技术还在不断突破创新，新的应用场景百花齐放。市场格局仍有很大的演变空间，在一些细分赛道仍有可能培育出独角兽型企业。与此同时，市场上规模化效应日益明显，早期的市场进入者都应该建立聚焦的战略路线，聚焦在重点业务领域积累客户群体以及实践案例，以尽早形成规模效应。”企业层面，商汤、旷视、海康威视、创新奇智、云从科技作为Top5的厂商，构成了45.6%的市场份额。在未列出的其他厂商中，百度智能云、阿里云、华为云、腾讯云也贡献了一定的市场份额。语音语义市场2021H2相比2020H2增长37.0%，市场规模达12.3亿美元，全年实现21.7亿美元的市场规模。市场增长驱动力来源于NLP技术的成熟以及相关应用场景的增长，智能语音以及对话式AI子市场均已进入缓慢增长期。整体格局越来越聚焦在头部厂商中。从年度数据来看，科大讯飞仍然位居第一，但市场份额有所下降，阿里云、百度智能云市场份额开始上升。在其他厂商中，华为云、京东云贡献了一定的市场份额，其次是对话式AI厂商、智能客服厂商。机器学习平台市场2021H2相比2020H2增长37.0%，市场规模达3.2亿美元，2021全年实现5.7亿美元。市场增长驱动力来源于政企构建AI中台，也来源于厂商端对于产品功能的不断完善以及在降低机器学习开发门槛方面所做的举措。从市场格局端，第四范式仍保持领先优势；华为云也取得高速增长；九章云极DataCanvas凭借其扎实的客户积累及不断创新能力、创新奇智依托其不断扩展的产品组合，市场份额也在不断上升。众所周知，AIoT是人工智能和物联网的融合应用，两种技术通过融合获益。随着AI技术的发展，AI技术将智能下沉到物联网系统的边缘，即传感器、相机、移动设备等硬件中。AI与IoT的融合，将数据分析移至IoT设备本身，从而消除了处理过程中的任何延迟。物联网的边缘设备不但能感测环境数据，透过深度学习等人工智能技术，设备能辨识周遭信息，将物联网进化成智慧物联网。AI让IoT拥有“大脑”，IoT给予AI“沃土”。这种相辅相成的合作，需要以“生态圈”的形式保持紧密联系。为了帮助更多的电子企业抓住这波AIoT市场发展商机，帮助其建立自己的生态圈或进入其它平台型企业生态圈，全球领先的专业电子机构媒体AspenCore将于2022年6月29日在深圳与深圳市新一代信息通信产业集群一起联合主办，推动工业互联网、智慧家庭、智慧机器人、智慧两轮车、智慧可穿戴等行业上下游企业之间的技术交流与商业合作，整合国际国内AIoT领域的技术力量与市场渠道资源，打造粤港澳大湾区电子产业链上下游企业发现未来独角兽企业、挖掘更合适技术合作伙伴、对接生态合作平台、拓展市场渠道资源的核心枢纽平台。据市调机构日前报告数据显示，在疫情趋缓驱使全球各类经济活动复苏的趋势下，2021年全球LED产值表现高于市场预期，达176.5亿美元，年增15.4%...美国半导体行业协会（SIA）日前在官网就电子传输征收关税问题发表了看法。据悉，世贸组织第12届部长级会议将于6月中旬在日内瓦举行。届时，作为全球数字贸易基础，存在四分之一世纪的电子传输征收关税问题将在一周内确定。随着物联网技术渗透千家万户，90后成为消费主力，懒人经济风潮下，代替人力、解放双手的电子产品成消费必然趋势。特别是2022年上半年，根据疫情防控的需要，“宅生活”时间增长，使人们更加注重家庭卫生清洁，为扫地机器人带来了新的市场机遇。我们都听说过汽车芯片短缺，这也许就是一些二手车的价值仍是几年前的两倍的原因。一般来说，当讨论芯片短缺时，即使是在汽车以外的行业，讨论的焦点都是MCU...国际电子商情6日获悉，尽管在当今这个通货膨胀、能源成本飙升、供应链持续出现故障、俄乌冲突的大背景下，市调机构仍在最新的报告中预测，今年半导体总销售额将增长11%——与去年同期的增长率相同，并将达到创纪录的6807亿美元销售额...PC这个门类的消费品，借助新冠所致居家办公、在家上课的热潮，过去两年的发展算是异军突起的。在连续2年多的时间里，PC出货量持续稳步提升，让这个原本在众人眼中已经十分成熟的市场重新焕发了活力。据市调机构Canalys最新发布报告显示，在2022年第一季度，印度个人电脑市场（包括台式机、笔记本电脑和平板电脑）增长，市调机构在最新的报告中指出，2021年全球平板电脑应用处理器市场规模增长了12%，达到30亿美元。苹果、英特尔、高通、联发科和三星LSI占据了2021年平板电脑应用处理器收益份额前五名。充电桩也需要跟上脚步。从充电技术、市场格局、产业链构成三个层次看，当下新能源汽车“补能战”已至中章。《国际电子商情》26日讯，根据IDC最新预测数据，2021年全球AR/VR总投资规模接近146.7亿美元，并有望在2026年增至747.3亿美元，五年复合增长率（CAGR）将达38.5%。其中，中国市场五年CAGR预计将达43.8%，增速位列全球第一。据市调机构最新报告数据显示，随着原厂积极移转产能向128层迈进，市场转向供过于求，导致本季合约价下跌，其中以消费级产品跌幅较为明显。外媒报导，随着制造技术越来越复杂，发展、研究和开发时间也越来越长。不再看到台积电和其他代工厂每两年就会出据TrendForce集邦咨询研究显示，尽管消费性电子需求持续疲弱，但服务器、高性能运算、车用与工控等领域产业结构2020年第四季度爆发的“缺芯”危机，令不少芯片产品价格上涨，模拟芯片由于产品种类繁杂，下游涉及市场广，应用需求5月17日，全球首款通过eSIM技术，实现4G手机秒变5G的手机壳“5G通信壳”正式发布，首批适配华为P506月16日，芯视佳12英寸硅基OLED微显示器制造项目签约仪式在安徽淮南高新区举行，项目总投资65亿元。坂本幸雄作为日本半导体产业的领袖，曾任德州仪器日本公司副社长，神户制钢电子信息科技半导体部门总监理，联日半2021年，安森美专注于其推动创新的使命，创建智能电源和智能感知技术，解决最具挑战的客户问题，同时强调致力于为明Qorvo、汇顶科技和深圳通公司已经成功联合演示了“无感过闸+多物理路线通行识别”集成系统。得益于UWB本身从汽车到灯泡，所有产品对电子元器件的需求都在上升，而且一眼望不到尽头。虽然元器件供应商正在扩大产能，但是整2022年6月8日，安森美推出了两个完整的系统方案，支持楼宇自动化网络协议——以太网供电(PoE)和KNX。从管理化工园区内的工作人员、追踪牛羊群的定位，到物流货运中实时获取货物的位置信息……物联网定位功能被越近日，由中国电子信息产业集团有限公司（简称“中国电子”）与中国科学院深圳先进技术研究院联合主办（简称“中科院后摩智能创始人兼CEO吴强博士表示，此次业内首款存算一体大算力AI芯片点亮，是对后摩技术和工程落地能力最好的我司杂志提供免费订阅，任何第三方平台的赠送或售卖行为均未获得我司授权，我司保留追究其法律责任的权利！

